{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SianC7/LAIDS/blob/main/Sian's_KPCA_1DCNN_(binary_classification).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1hL6nmN67V1"
      },
      "source": [
        "#Extract features from pcap files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXAAO45a6_QV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOSaPtDFUdwR"
      },
      "source": [
        "#IoT-23 Data preprocessing\n",
        "- Collection\n",
        "- Cleaning\n",
        "- Encoding\n",
        "- Scaling\n",
        "- Normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfqR0Okmzqu-"
      },
      "source": [
        "1. Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9MBfihKzudi"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# # Set pandas display options for wide output\n",
        "# pd.set_option('display.max_columns', None)\n",
        "# pd.set_option('display.width', None)\n",
        "# pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# # Data Collection\n",
        "\n",
        "# data_file_path = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 8GB Dataset/CTU-IoT-Malware-Capture-35-1-log.txt'\n",
        "# #Read the file\n",
        "# df = pd.read_csv(\n",
        "#     data_file_path,\n",
        "#     sep=\"\\t\",\n",
        "#     comment=\"#\",\n",
        "#     header=None,\n",
        "#     names=[\n",
        "#         \"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\", \"proto\",\n",
        "#         \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\", \"conn_state\",\n",
        "#         \"local_orig\", \"local_resp\", \"missed_bytes\", \"history\", \"orig_pkts\",\n",
        "#         \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\", \"label\"\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# # # --------------------------------------------------------------------------------------------------------------------------------------\n",
        "# # #TEST:\n",
        "# # # List of file paths\n",
        "# # data_file_paths = [\n",
        "# #     '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/Benign_Dataset13.csv',\n",
        "# #     '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/DoS_Dataset21.csv',\n",
        "# #     '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/GafgytBotnet_Dataset3.csv',\n",
        "# #     '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/MiraiBotnet_Dataset1.csv',\n",
        "# #     '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/MiraiVariant_Dataset5.csv',\n",
        "# #     '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/OkiruBotnet_Dataset17.csv',\n",
        "# #     '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/PartOfMioriBotnet_Dataset20.csv'\n",
        "\n",
        "# # ]\n",
        "# # dfs = []#set the df list var\n",
        "\n",
        "# # for file_path in data_file_paths: # Loop through the file paths and add the data to a temp df var\n",
        "# #   try:\n",
        "# #       temp_df = pd.read_csv(\n",
        "# #           file_path,\n",
        "# #           sep=\",\",\n",
        "# #           comment=\"#\",\n",
        "# #           header=0,# Use the first non-comment line as the header\n",
        "# #           names=[\n",
        "# #               \"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\", \"proto\",\n",
        "# #               \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\", \"conn_state\",\n",
        "# #               \"local_orig\", \"local_resp\", \"missed_bytes\", \"history\", \"orig_pkts\",\n",
        "# #               \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\", \"label\"\n",
        "# #           ]\n",
        "# #       )\n",
        "# #       dfs.append(temp_df) #add the info in the temp df to the dfs list\n",
        "# #       print(f\"Loaded file: {file_path} with {len(temp_df)} rows\")\n",
        "# #   except Exception as e:\n",
        "# #       print(f\"Error reading file {file_path}: {e}\")\n",
        "\n",
        "# # # Concatenate all dataframes into one\n",
        "# # df = pd.concat(dfs, ignore_index=True) #make the list of dataframes into one big dataframe\n",
        "# # # --------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# print(\"\\nInitial sample:\")\n",
        "# print(df.head().to_string())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set pandas display options for wide output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# ---- Data Extraction ----\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/CICIDS2017_combined.csv'\n",
        "#Read the file\n",
        "CICIDS2017_df = pd.read_csv(file_path, sep=\",\", comment=\"#\", header=0)\n",
        "CICIDS2017_df.columns = CICIDS2017_df.columns.str.strip()  # Strip whitespace from column names\n",
        "\n",
        "print(f\"Loaded file: {file_path} with {len(CICIDS2017_df)} rows\")\n",
        "\n",
        "print(\"\\nInitial sample:\")\n",
        "print(CICIDS2017_df.head().to_string())\n",
        "print(CICIDS2017_df.info())"
      ],
      "metadata": {
        "id": "tSXTvQyKvREw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSzslsd9zu-V"
      },
      "source": [
        "2. Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9S8k3TYGvIBW",
        "outputId": "1c4a4b72-5d1e-4d03-cd83-50f7532310b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cleaned sample:\n",
            "                             ts  id.orig_p  id.resp_p  proto  service  duration  orig_bytes  resp_bytes  conn_state  missed_bytes  orig_pkts  orig_ip_bytes  resp_pkts  resp_ip_bytes  label\n",
            "0 2018-12-21 14:34:02.863611937      59932         80      1        3  3.097754           0           0           6             0          3            180          0              0      1\n",
            "3 2018-12-21 14:34:13.913069010      35883         53      2        1  5.005148          78           0           6             0          2            134          0              0      0\n",
            "4 2018-12-21 14:34:03.902539968      43531         53      2        1  5.005145          78           0           6             0          2            134          0              0      0\n",
            "6 2018-12-21 14:34:47.880949974      39189         53      2        1  5.005149          78           0           6             0          2            134          0              0      0\n",
            "7 2018-12-21 14:34:11.880244970        123        123      2        3  0.143648          48          48          10             0          1             76          1             76      0\n",
            "\n",
            "Data types and nulls:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 6015172 entries, 0 to 10447784\n",
            "Data columns (total 15 columns):\n",
            " #   Column         Dtype         \n",
            "---  ------         -----         \n",
            " 0   ts             datetime64[ns]\n",
            " 1   id.orig_p      int64         \n",
            " 2   id.resp_p      int64         \n",
            " 3   proto          int64         \n",
            " 4   service        int64         \n",
            " 5   duration       float64       \n",
            " 6   orig_bytes     int64         \n",
            " 7   resp_bytes     int64         \n",
            " 8   conn_state     int64         \n",
            " 9   missed_bytes   int64         \n",
            " 10  orig_pkts      int64         \n",
            " 11  orig_ip_bytes  int64         \n",
            " 12  resp_pkts      int64         \n",
            " 13  resp_ip_bytes  int64         \n",
            " 14  label          int64         \n",
            "dtypes: datetime64[ns](1), float64(1), int64(13)\n",
            "memory usage: 734.3 MB\n",
            "None\n",
            "\n",
            "Total logs: 6015172\n",
            "Malware logs: 1906832\n",
            "Benign (non-malicious) logs: 4108340\n",
            "\n",
            "Total logs: 6015172\n",
            "Malware logs: 1906832\n",
            "Benign (non-malicious) logs: 4108340\n"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# # Set pandas display options for wide output\n",
        "# pd.set_option('display.max_columns', None)\n",
        "# pd.set_option('display.width', None)\n",
        "# pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "# # Data Cleaning\n",
        "# # Convert timestamp to datetime\n",
        "# df[\"ts\"] = pd.to_datetime(df[\"ts\"], unit=\"s\", errors='coerce')\n",
        "\n",
        "# # Drop non-informative or redundant columns\n",
        "# df.drop(columns=['uid', 'id.orig_h', 'id.resp_h', 'local_orig', 'local_resp', 'history'], inplace=True)\n",
        "\n",
        "# # Replace '-' with NaN for consistent handling\n",
        "# df.replace('-', np.nan, inplace=True)\n",
        "\n",
        "# # Drop rows where critical numeric fields are missing\n",
        "# df.dropna(subset=[\n",
        "#     'duration', 'orig_bytes', 'resp_bytes', 'id.orig_p', 'id.resp_p',\n",
        "#     'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes'\n",
        "# ], inplace=True)\n",
        "\n",
        "# # Convert appropriate columns to numeric types\n",
        "# numeric_columns = [\n",
        "#     'duration', 'orig_bytes', 'resp_bytes', 'id.orig_p', 'id.resp_p',\n",
        "#     'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'missed_bytes'\n",
        "# ]\n",
        "# for col in numeric_columns:\n",
        "#     df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "# # Encode categorical columns\n",
        "# categorical_columns = ['proto', 'conn_state', 'service']\n",
        "# for col in categorical_columns:\n",
        "#     df[col] = df[col].astype(str)  # Ensure strings\n",
        "#     df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# # Convert label to binary (malicious=1, benign=0)\n",
        "# df['label'] = df['label'].apply(lambda x: 1 if 'Malicious' in str(x) else 0)\n",
        "\n",
        "# # Drop any rows still containing NaN\n",
        "# df.dropna(inplace=True)\n",
        "\n",
        "# print(\"\\nCleaned sample:\")\n",
        "# print(df.head().to_string())\n",
        "# print(\"\\nData types and nulls:\")\n",
        "# print(df.info())\n",
        "\n",
        "# # --- Add counts of malware and benign logs ---\n",
        "# total_logs = len(df)\n",
        "# malware_logs = df['label'].sum()  # since malware=1\n",
        "# benign_logs = total_logs - malware_logs\n",
        "\n",
        "# print(f\"\\nTotal logs: {total_logs}\")\n",
        "# print(f\"Malware logs: {malware_logs}\")\n",
        "# print(f\"Benign (non-malicious) logs: {benign_logs}\")\n",
        "\n",
        "\n",
        "# # --- Split features and labels ---\n",
        "# X = df.drop(columns=['label', 'ts']).values  # Drop label and timestamp\n",
        "# y = df['label'].values\n",
        "\n",
        "# # --- Train/Validation/Test Split --- #TEST\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.utils import shuffle\n",
        "# # Split test set (20%)\n",
        "# X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# # Split validation set (20% of remaining)\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n",
        "\n",
        "\n",
        "# # Shuffle data\n",
        "# X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
        "# X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
        "# X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
        "\n",
        "# # --- Normalize ---\n",
        "# # X_scaled = StandardScaler().fit_transform(X) #standardise data features\n",
        "# #TEST\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)  # Calculate the mean and standard deviation of each feature in the training set\n",
        "# X_val_scaled = scaler.transform(X_val)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# # --- Add counts of malware and benign logs ---\n",
        "# total_logs = len(df)\n",
        "# malware_logs = df['label'].sum()  # since malware=1\n",
        "# benign_logs = total_logs - malware_logs\n",
        "\n",
        "# print(f\"\\nTotal logs: {total_logs}\")\n",
        "# print(f\"Malware logs: {malware_logs}\")\n",
        "# print(f\"Benign (non-malicious) logs: {benign_logs}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set pandas display options for wide output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# ---- Data Cleaning ----\n",
        "\n",
        "\n",
        "# Replace '-' with NaN for consistent handling\n",
        "CICIDS2017_df.replace('-', np.nan, inplace=True)\n",
        "\n",
        "# # Encode categorical column\n",
        "# categorical_columns = ['Label']\n",
        "# for col in categorical_columns:\n",
        "#     CICIDS2017_df[col] = CICIDS2017_df[col].astype(str)  # Ensure strings\n",
        "#     CICIDS2017_df[col] = LabelEncoder().fit_transform(CICIDS2017_df[col])\n",
        "\n",
        "# Convert label to binary (malicious=1, benign=0)\n",
        "CICIDS2017_df['Label'] = CICIDS2017_df['Label'].apply(lambda x: 0 if 'BENIGN' in str(x) else 1)\n",
        "\n",
        "# Drop any rows still containing NaN\n",
        "CICIDS2017_df.dropna(inplace=True)\n",
        "\n",
        "print(\"\\nCleaned sample:\")\n",
        "print(CICIDS2017_df.head().to_string())\n",
        "print(CICIDS2017_df.info())\n",
        "\n",
        "# --- Add counts of malware and benign logs ---\n",
        "total_logs = len(CICIDS2017_df)\n",
        "malware_logs = CICIDS2017_df['Label'].sum()  # since malware=1\n",
        "benign_logs = total_logs - malware_logs\n",
        "\n",
        "print(f\"\\nTotal logs: {total_logs}\")\n",
        "print(f\"Malware logs: {malware_logs}\")\n",
        "print(f\"Benign (non-malicious) logs: {benign_logs}\")"
      ],
      "metadata": {
        "id": "otdGLxE7vWsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK_BwR0q-sNG"
      },
      "source": [
        "#Visualise dataset (Don't use for large data samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WMY95d7M-uoK"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import plotly.express as px\n",
        "\n",
        "# # List of feature columns to visualize\n",
        "# features = [\n",
        "#     \"id.orig_p\", \"id.resp_p\", \"proto\", \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\",\n",
        "#     \"conn_state\", \"missed_bytes\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\"\n",
        "# ]\n",
        "\n",
        "# # Map label values to readable categories if label is 0/1\n",
        "# df[\"label\"] = df[\"label\"].map({0: \"Benign\", 1: \"Malicious\"})\n",
        "\n",
        "# # Create scatter matrix plot\n",
        "# fig = px.scatter_matrix(\n",
        "#     df,\n",
        "#     dimensions=features,\n",
        "#     color=\"label\",  # Color-code by label\n",
        "#     title=\"Scatter Matrix of Network Traffic Features\",\n",
        "#     width=1800,  # Increase width\n",
        "#     height=1800,  # Increase height\n",
        "# )\n",
        "\n",
        "# # Update marker and layout to reduce overlap\n",
        "# fig.update_traces(\n",
        "#     diagonal_visible=False,\n",
        "#     marker=dict(size=3, opacity=0.6)  # Smaller, semi-transparent points\n",
        "# )\n",
        "\n",
        "# # Show interactive plot\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmyyy17MvcTO"
      },
      "source": [
        "Apply PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UmtAu4jtvdNy",
        "outputId": "5b9e6d26-a1fa-457e-a705-c2c7dd2e714e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "principle components: 9\n"
          ]
        }
      ],
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# # Make an instance of the pca Model\n",
        "# pca = PCA(.95) # sklearn chooses the minimum number of principal components such that 95 percent of the variance in the data is retained\n",
        "\n",
        "# # Only fit the PCA to the training data\n",
        "# pcaX_train = pca.fit_transform(X_train_scaled)#To avoid data leakage (information from the test set influences the model during training)\n",
        "# print(f\"principle components: {pca.n_components_}\")\n",
        "\n",
        "# pcaX_val = pca.transform(X_val_scaled)\n",
        "# pcaX_test = pca.transform(X_test_scaled)# Apply the PCA to the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZCG-d2QNxzG"
      },
      "source": [
        "#Apply KPCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x16g46X7N0tz"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Make an instance of the kpca model\n",
        "kernel_pca = KernelPCA(n_components=5, kernel=\"rbf\", gamma=1) #explain!!! #10, gamma = 1\n",
        "\n",
        "# Only fit the PCA to the training data\n",
        "kpcaX_train = kernel_pca.fit_transform(pcaX_train)#To avoid data leakage (information from the test set influences the model during training)\n",
        "print(f\"principle components: {kernel_pca.n_components}\")\n",
        "\n",
        "kpcaX_val = kernel_pca.transform(pcaX_val)\n",
        "kpcaX_test = kernel_pca.transform(pcaX_test)# Apply the PCA to the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj1szLf679u6"
      },
      "source": [
        "#Visualise KPCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeE2Fj4c8JXc"
      },
      "outputs": [],
      "source": [
        "# import plotly.express as px\n",
        "# import pandas as pd\n",
        "\n",
        "# kpcaX_train_features = []\n",
        "\n",
        "# # Access n_components directly from the kernel_pca object\n",
        "# for i in range(kernel_pca.n_components):\n",
        "#   kpcaX_train_features.append(f'KPC{i+1}')\n",
        "\n",
        "# print(f\"KPCA features: {kpcaX_train_features}\")\n",
        "\n",
        "# # Create a DataFrame from the PCA results with column names\n",
        "# kpca_train_df = pd.DataFrame(data=kpcaX_train, columns=kpcaX_train_features)\n",
        "\n",
        "# # Add the original labels for coloring\n",
        "# kpca_train_df['label'] = y_train\n",
        "\n",
        "# fig = px.scatter_matrix(\n",
        "#     kpca_train_df,\n",
        "#     dimensions= kpcaX_train_features,\n",
        "#     color=\"label\"\n",
        "# )\n",
        "# fig.update_traces(diagonal_visible=False)\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvPeSETFDnXG"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import plotly.express as px\n",
        "\n",
        "\n",
        "# # Make sure labels are strings \"Mal\" and \"Normal\"\n",
        "# # If y_train is numeric (e.g., 0,1), map it:\n",
        "# label_map = {0: 'Benign', 1: 'Malicious'}\n",
        "# kpca_train_df['label'] = [label_map[label] if label in label_map else label for label in y_train]\n",
        "\n",
        "# fig = px.scatter(\n",
        "#     kpca_train_df,\n",
        "#     x='KPC1',\n",
        "#     y='KPC2',\n",
        "#     color='label',\n",
        "#     title=f'KPCA Scatter Plot: KPC1 & KPC2',\n",
        "#     labels={\n",
        "#         'KPC1': f'KPC1',\n",
        "#         'KPC2': f'KPC2',\n",
        "#         'label': 'Label'\n",
        "#     }\n",
        "# )\n",
        "\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBxWX3MftapB"
      },
      "source": [
        "# Data reshaping for 1D CNN input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0JKdNVLteDk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# reshape the data so it can be used as input for the 1D CNN\n",
        "X_train = kpcaX_train.reshape((kpcaX_train.shape[0], kpcaX_train.shape[1], 1))\n",
        "X_val = kpcaX_val.reshape((kpcaX_val.shape[0], kpcaX_val.shape[1], 1))\n",
        "X_test = kpcaX_test.reshape((kpcaX_test.shape[0], kpcaX_test.shape[1], 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P41qC7snwZcA"
      },
      "source": [
        "#Visualisation of data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMLMgNYMwc0Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"Input Shapes -> Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "print(f\"Labels[Benign, Mal] -> Train: {np.bincount(y_train)}, Val: {np.bincount(y_val)}, Test: {np.bincount(y_test)}\")\n",
        "\n",
        "train_counts = np.bincount(y_train)\n",
        "val_counts = np.bincount(y_val)\n",
        "test_counts = np.bincount(y_test)\n",
        "\n",
        "# Data for plotting\n",
        "labels = ['Benign', 'Malicious']\n",
        "datasets = ['Train', 'Validation', 'Test']\n",
        "counts = [train_counts, val_counts, test_counts]\n",
        "\n",
        "# Transpose counts to group by label\n",
        "benign_counts = [c[0] for c in counts]\n",
        "mal_counts = [c[1] for c in counts]\n",
        "\n",
        "x = np.arange(len(datasets))  # the label locations\n",
        "width = 0.35  # width of the bars\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, benign_counts, width, label='Benign')\n",
        "rects2 = ax.bar(x + width/2, mal_counts, width, label='Malicious')\n",
        "\n",
        "# Add labels\n",
        "ax.set_ylabel('Number of Samples')\n",
        "ax.set_title('Label Distribution Across Datasets')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(datasets)\n",
        "ax.legend()\n",
        "\n",
        "# Add value labels\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height}',\n",
        "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
        "                    xytext=(0, 3),  # vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaK_AOGeuPxj"
      },
      "source": [
        "#1D CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z09AdYcuSTj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# Define the 1D CNN Model\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (X_train.shape[1], 1)  # (timesteps, features)\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape), # Detect patterns in the network traffic data\n",
        "    BatchNormalization(), # Normalizes the outputs of a the Conv1D layer before passing them to the MaxPool layer\n",
        "    MaxPooling1D(pool_size=2), # Reduce the dimensions of the data without affecting key features\n",
        "    Dropout(0.25), # Prevent overfitting by forcing the model to generalize - it does this by randomly deactivating a fraction of neurons during training\n",
        "\n",
        "    Conv1D(filters=64, kernel_size=2, activation='relu'),#kernel_size=3\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    Flatten(), # Converts the output of the last Conv1D layer into a 1D vector for the fully connected layers\n",
        "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)), # Apply L2 regularisation to prevent overfitting (common in the dense layer) #https://medium.com/@bhatadithya54764118/day-49-overfitting-and-underfitting-in-dl-regularization-techniques-8ded20baa3d6\n",
        "    Dropout(0.5), # Randomly drop 50% of the network;s neurons to further prevent overfitting\n",
        "    Dense(1, activation='sigmoid')  # Final output layer( 1 = 1 neuron for binary classification, sigmoid = decides if input is malicious (1) or benign (0))\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Loss function measures how well the modelâ€™s predictions match true labels\n",
        "\n",
        "#Summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5WWCRHmusAX"
      },
      "source": [
        "#Train, Evaluate (Test) & Visualize 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGslPs6rurxy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train the Model\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot accuracy and loss\n",
        "plt.figure(figsize=(12,5)) # Create a new figure that is 12x5\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR3T_G90vSvu"
      },
      "source": [
        "# Evaluate 1D CNN Malware Detection Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1chgEQVvU-s"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# Get predictions of x_test dataset\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int).flatten()  # Convert probabilities to 0 or 1, 0.5 = Threshold\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Optional: print as table\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Detailed breakdown\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"\\nTrue Negatives (Benign correctly classified): {tn}\")\n",
        "print(f\"False Positives (Benign misclassified as malware): {fp}\")\n",
        "print(f\"False Negatives (Malware missed): {fn}\")\n",
        "print(f\"True Positives (Malware correctly identified): {tp}\")\n",
        "\n",
        "# Accuracy scores\n",
        "print(\"Accuracy:\")\n",
        "print(\"sklearn Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)# Evaluate model with test set\n",
        "print(f\"model.evaluate Accuracy: {test_acc:.4f}\")\n",
        "print(f\" Confusion Matrix Accuracy: {(tp + tn) / (tp + tn + fp + fn)}\")\n",
        "\n",
        "# Classification report (accuracy, precision, recall, F1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Benign\", \"Malicious\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q86I60okv_0b"
      },
      "source": [
        "#Visualisation of results ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8IYgq3ywHDJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# Create x-axis index\n",
        "x = np.arange(len(y_pred_probs))\n",
        "\n",
        "# y_pred: predicted labels from the model (e.g., [0, 1, 1, 0])\n",
        "# y_test: true labels (e.g., [0, 1, 0, 1])\n",
        "\n",
        "# Check if each prediction is correct\n",
        "correct_predictions = y_pred == y_test # If y_pred[i] == y_test[i] then correct_predictions = True, else correct_predictions = False\n",
        "colors = []\n",
        "for is_correct in correct_predictions: # If correct_predictions is True, use 'green' else use 'red'\n",
        "    if is_correct:\n",
        "        colors.append('green')  # Correct prediction\n",
        "    else:\n",
        "        colors.append('red')    # Incorrect prediction\n",
        "\n",
        "# Make scatter plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.scatter(x, y_pred_probs, c=colors, alpha=0.6, s=20) #plot a point (y_pred_probs), if the y_pred is true for that point, then colour the point green\n",
        "plt.axhline(y=0.5, color='black', linestyle='--', linewidth=1.5)\n",
        "\n",
        "# Add combined legend\n",
        "legend_elements = [\n",
        "    Patch(facecolor='green', label='Correct Prediction'),\n",
        "    Patch(facecolor='red', label='Incorrect Prediction'),\n",
        "    Line2D([0], [0], color='black', lw=2, linestyle='--', label='Threshold = 0.5')\n",
        "]\n",
        "plt.legend(handles=legend_elements)\n",
        "\n",
        "# Labels and styling\n",
        "plt.title(\"Predictions\")\n",
        "plt.xlabel(\"Sample Index\") # position of each test sample in the X_test dataset\n",
        "plt.ylabel(\"Predicted Probability\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1WmZHCYiS-YYO0Koe3UMh5u0KMtnbt5eR",
      "authorship_tag": "ABX9TyORkM4Js4rQeQNZznYO4lOW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}