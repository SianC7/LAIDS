{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Q5tvrHcbYw08rfFsT176D5S8tvKKnNbc",
      "authorship_tag": "ABX9TyMWyyGVLZ/+P6OTUL5KBNSg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SianC7/LAIDS/blob/main/Sian's_PCA_1DCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IoT-23 Data preprocessing\n",
        "- Collection\n",
        "- Cleaning\n",
        "- Encoding\n",
        "- Scaling\n",
        "- Normalisation"
      ],
      "metadata": {
        "id": "oOSaPtDFUdwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Set pandas display options for wide output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Step 1: Data Collection\n",
        "#data_file_path = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 8GB Dataset/CTU-IoT-Malware-Capture-1-1-log.txt'\n",
        "# Read the file\n",
        "# df = pd.read_csv(\n",
        "#     data_file_path,\n",
        "#     sep=\"\\t\",\n",
        "#     comment=\"#\",\n",
        "#     header=None,\n",
        "#     names=[\n",
        "#         \"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\", \"proto\",\n",
        "#         \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\", \"conn_state\",\n",
        "#         \"local_orig\", \"local_resp\", \"missed_bytes\", \"history\", \"orig_pkts\",\n",
        "#         \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\", \"label\"\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "#TEST:\n",
        "# List of file paths\n",
        "data_file_paths = [\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/Benign_Dataset13.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/DoS_Dataset21.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/GafgytBotnet_Dataset3.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/MiraiBotnet_Dataset1.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/MiraiVariant_Dataset5.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/OkiruBotnet_Dataset17.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/PartOfMioriBotnet_Dataset20.csv'\n",
        "\n",
        "]\n",
        "dfs = []#set the df list var\n",
        "\n",
        "for file_path in data_file_paths: # Loop through the file paths and add the data to a temp df var\n",
        "  try:\n",
        "      temp_df = pd.read_csv(\n",
        "          file_path,\n",
        "          sep=\",\",\n",
        "          comment=\"#\",\n",
        "          header=0,# Use the first non-comment line as the header\n",
        "          # names=[\n",
        "          #     \"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\", \"proto\",\n",
        "          #     \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\", \"conn_state\",\n",
        "          #     \"local_orig\", \"local_resp\", \"missed_bytes\", \"history\", \"orig_pkts\",\n",
        "          #     \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\", \"label\"\n",
        "          # ]\n",
        "      )\n",
        "      dfs.append(temp_df) #add the info in the temp df to the dfs list\n",
        "      print(f\"Loaded file: {file_path} with {len(temp_df)} rows\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error reading file {file_path}: {e}\")\n",
        "\n",
        "# Concatenate all dataframes into one\n",
        "df = pd.concat(dfs, ignore_index=True) #make the list of dataframes into one big dataframe\n",
        "\n",
        "\n",
        "print(\"\\nInitial sample:\")\n",
        "print(df.head().to_string())\n",
        "\n",
        "# Step 2: Data Cleaning\n",
        "# Convert timestamp to datetime\n",
        "df[\"ts\"] = pd.to_datetime(df[\"ts\"], unit=\"s\", errors='coerce')\n",
        "\n",
        "# Drop non-informative or redundant columns\n",
        "df.drop(columns=['uid', 'id.orig_h', 'id.resp_h', 'local_orig', 'local_resp', 'history'], inplace=True)\n",
        "\n",
        "# Replace '-' with NaN for consistent handling\n",
        "df.replace('-', np.nan, inplace=True)\n",
        "\n",
        "# Drop rows where critical numeric fields are missing\n",
        "df.dropna(subset=[\n",
        "    'duration', 'orig_bytes', 'resp_bytes', 'id.orig_p', 'id.resp_p',\n",
        "    'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes'\n",
        "], inplace=True)\n",
        "\n",
        "# Convert appropriate columns to numeric types\n",
        "numeric_columns = [\n",
        "    'duration', 'orig_bytes', 'resp_bytes', 'id.orig_p', 'id.resp_p',\n",
        "    'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'missed_bytes'\n",
        "]\n",
        "for col in numeric_columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "# Encode categorical columns\n",
        "categorical_columns = ['proto', 'conn_state', 'service']\n",
        "for col in categorical_columns:\n",
        "    df[col] = df[col].astype(str)  # Ensure strings\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Convert label to binary (malicious=1, benign=0)\n",
        "df['detailed-label'] = df['detailed-label'].apply(lambda x: 1 if 'Malicious' in str(x) else 0)\n",
        "\n",
        "# Drop any rows still containing NaN\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(\"\\nCleaned sample:\")\n",
        "print(df.head().to_string())\n",
        "print(\"\\nData types and nulls:\")\n",
        "print(df.info())\n",
        "# print(\"\\nStatistics:\")\n",
        "# print(df.describe().to_string())\n",
        "\n",
        "\n",
        "# --- Split features and labels ---\n",
        "X = df.drop(columns=['detailed-label', 'ts']).values  # Drop label and timestamp\n",
        "y = df['detailed-label'].values\n",
        "\n",
        "# --- Normalize ---\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "\n",
        "# --- Add counts of malware and benign logs ---\n",
        "total_logs = len(df)\n",
        "malware_logs = df['detailed-label'].sum()  # since malware=1\n",
        "benign_logs = total_logs - malware_logs\n",
        "\n",
        "print(f\"\\nTotal logs: {total_logs}\")\n",
        "print(f\"Malware logs: {malware_logs}\")\n",
        "print(f\"Benign (non-malicious) logs: {benign_logs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "9S8k3TYGvIBW",
        "outputId": "746bf306-6b24-4c2c-9087-1535e0ca8691"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/Benign_Dataset13.csv with 4426 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/DoS_Dataset21.csv with 3209 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/GafgytBotnet_Dataset3.csv with 130 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/MiraiBotnet_Dataset1.csv with 452 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/MiraiVariant_Dataset5.csv with 10403 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/OkiruBotnet_Dataset17.csv with 23145 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/IoT-23 Preprocessed Dataset/PartOfMioriBotnet_Dataset20.csv with 3286 rows\n",
            "\n",
            "Initial sample:\n",
            "             ts                 uid      id.orig_h  id.orig_p      id.resp_h  id.resp_p proto service  duration orig_bytes resp_bytes conn_state local_orig local_resp  missed_bytes     history  orig_pkts  orig_ip_bytes  resp_pkts  resp_ip_bytes tunnel_parents   label   detailed-label\n",
            "0  1.547127e+09  CXY5uG2sSmjJ0grfY2  192.168.1.197      58312  104.24.96.120         80   tcp    http  3.909013         83      67212         SF          -          -          4380  ShADadttFf         54           3371         50          69224            -   Malicious   FileDownload\n",
            "1  1.547127e+09   Ce3AJzwzXwM3Z1XBg  192.168.1.197      45082  104.24.97.120         80   tcp    http  4.767024        150      67212         SF          -          -             0  ShADadttFf         54           2938         50          69224        -   Malicious   C&C-FileDownload\n",
            "2  1.547127e+09  CJgnSb3XpbbOcMHKUd  192.168.1.197      58316  104.24.96.120         80   tcp       -  3.107228          0          0         S0          -          -             0           S          3            180          0              0                          -   Benign   -\n",
            "3  1.547127e+09   Cq43w4aHlsW8nXZ3l  192.168.1.197      59357    192.168.1.1         53   udp     dns  0.029483         58        146         SF          -          -             0          Dd          2            114          2            202                          -   Benign   -\n",
            "4  1.547127e+09  C5uLwl2hGy10y9PSr6  192.168.1.197      39686    192.168.1.1         53   udp     dns  0.001249         58        146         SF          -          -             0          Dd          2            114          2            202                          -   Benign   -\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'detailed-label'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'detailed-label'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-192556631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# Convert label to binary (malicious=1, benign=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'detailed-label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'detailed-label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'Malicious'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# Drop any rows still containing NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'detailed-label'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apply PCA"
      ],
      "metadata": {
        "id": "p27J58TaT8nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=10) #PCA will reduce the dimensionality of the data to exactly 10 features\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "print(f\"Reduced {X_scaled.shape[1]} to {X_pca.shape[1]} components\")"
      ],
      "metadata": {
        "id": "jBS-NuKeT-hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data reshaping for 1D CNN input"
      ],
      "metadata": {
        "id": "WBxWX3MftapB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Step 1: Reshape the data for 1D CNN input\n",
        "# --- 1D CNN expects input shape: (samples, timesteps, features) ---\n",
        "# Here: timesteps = number of features, features = 1 (per timestep)\n",
        "X_1dcnn = X_pca.reshape((X_pca.shape[0], X_pca.shape[1], 1))\n",
        "\n",
        "#Step 2: Train/Validation/Test Split\n",
        "# --- Split into Train/Val/Test ---\n",
        "# Split test set (20%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_1dcnn, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# Split validation set (20% of remaining)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n",
        "# Shuffle data:\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
        "X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
        "X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
        "\n",
        "# --- Final Shape Confirmation ---\n",
        "print(\"1D CNN input shape:\", X_1dcnn.shape)\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "print(f\"Labels[Benign, Mal] - Train: {np.bincount(y_train)}, Val: {np.bincount(y_val)}, Test: {np.bincount(y_test)}\")"
      ],
      "metadata": {
        "id": "V0JKdNVLteDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1D CNN model"
      ],
      "metadata": {
        "id": "AaK_AOGeuPxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "#Step 3: Define the 1D CNN Model\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (X_train.shape[1], 1)  # (timesteps, features)\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape), # Detect patterns in the network traffic data\n",
        "    BatchNormalization(), # Normalizes the outputs of a the Conv1D layer before passing them to the MaxPool layer\n",
        "    MaxPooling1D(pool_size=2), # Reduce the dimensions of the data without affecting key features\n",
        "    Dropout(0.25), # Prevent overfitting by forcing the model to generalize - it does this by randomly deactivating a fraction of neurons during training\n",
        "\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    Flatten(), # Converts the output of the last Conv1D layer into a 1D vector for the fully connected layers\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5), # Randomly drop 50% of the network;s neurons to further prevent overfitting\n",
        "    Dense(1, activation='sigmoid')  # Final output layer( 1 = 1 neuron for binary classification, sigmoid = decides if input is malicious (1) or benign (0).\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Loss function measures how well the model’s predictions match true labels\n",
        "\n",
        "#Summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "5z09AdYcuSTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train, Evaluate (Test) & Visualize 1D CNN"
      ],
      "metadata": {
        "id": "h5WWCRHmusAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Step 4: Train the Model\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 5: Evaluate on Test Set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "# Step 6: Plot Accuracy and Loss\n",
        "plt.figure(figsize=(12,5)) # Create a new figure that is 12x5\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "yGslPs6rurxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate 1D CNN Malware Detection Results"
      ],
      "metadata": {
        "id": "xR3T_G90vSvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# Get predictions\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int).flatten()  # Convert probabilities to 0 or 1\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Optional: print as table\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Detailed breakdown\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"\\nTrue Negatives (Benign correctly classified): {tn}\")\n",
        "print(f\"False Positives (Benign misclassified as malware): {fp}\")\n",
        "print(f\"False Negatives (Malware missed): {fn}\")\n",
        "print(f\"True Positives (Malware correctly identified): {tp}\")\n",
        "\n",
        "# Accuracy score\n",
        "print(\"sklearn accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Classification report (accuracy, precision, recall, F1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Benign\", \"Malicious\"]))"
      ],
      "metadata": {
        "id": "G1chgEQVvU-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1D CNN Quant model"
      ],
      "metadata": {
        "id": "L3gfDVIU2grG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjeYR1Sf2j9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate int8 1D CNN"
      ],
      "metadata": {
        "id": "9Em5y-Cl26vO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "heu0aEfQ2_hA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}