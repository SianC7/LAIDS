{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SianC7/LAIDS/blob/main/1D_CNN_baseline_model(%2Bquant).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CICIDS2017 Dataset"
      ],
      "metadata": {
        "id": "DXZ5uI8bDP3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Data Collection ---\n",
        "\n",
        "# Set pandas display options for wide output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "# Get Data file path\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/cicids2017_cleaned.csv'\n",
        "cicids2017_df = pd.read_csv(file_path, sep=\",\", comment=\"#\", header=0)\n",
        "cicids2017_df.columns = cicids2017_df.columns.str.strip()  # Strip whitespace from column names\n",
        "\n",
        "\n",
        "print(\"\\nInitial samples:\")\n",
        "print(f\"cicids2017_df shape: {cicids2017_df.shape}\")\n",
        "print(cicids2017_df.head().to_string())\n",
        "print(cicids2017_df.info())\n",
        "\n",
        "# Print unique values and their counts for 'Attack Type'\n",
        "print(\"\\nAttack Type Distribution:\")\n",
        "print(cicids2017_df['Attack Type'].value_counts())\n",
        "\n",
        "# --- Label Encoding ---\n",
        "\n",
        "cicids2017_df['Attack Type'] = cicids2017_df['Attack Type'].apply(lambda x: 0 if x == 'Normal Traffic' else 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "F3zsgHCrDVBI",
        "outputId": "4e9e1c34-bc27-46dd-cf26-c32dca82edbe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/cicids2017_cleaned.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-645965722.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Get Data file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/cicids2017_cleaned.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcicids2017_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mcicids2017_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcicids2017_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Strip whitespace from column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/cicids2017_cleaned.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/val/test split"
      ],
      "metadata": {
        "id": "L_-3p8agD5ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# --- Train/val/test split ---\n",
        "# Split label from datafram\n",
        "X = cicids2017_df.drop('Attack Type', axis=1)\n",
        "y = cicids2017_df['Attack Type']\n",
        "\n",
        "# Split Data\n",
        "X_temp, X_test, y_temp, y_test= train_test_split(X, y, test_size=0.3, random_state=42, stratify = y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify = y_temp)\n",
        "\n",
        "# Shuffle the data\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
        "X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
        "X_test, y_test = shuffle(X_test, y_test, random_state=42)\n"
      ],
      "metadata": {
        "id": "n0CyZlVTD-QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisation of data split"
      ],
      "metadata": {
        "id": "pBlQLyboK9hQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"Input Shapes -> Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "print(f\"Labels[Benign, Mal] -> Train: {np.bincount(y_train)}, Val: {np.bincount(y_val)}, Test: {np.bincount(y_test)}\")\n",
        "\n",
        "train_counts = np.bincount(y_train)\n",
        "val_counts = np.bincount(y_val)\n",
        "test_counts = np.bincount(y_test)\n",
        "\n",
        "# Data for plotting\n",
        "labels = ['Benign', 'Malicious']\n",
        "datasets = ['Train', 'Validation', 'Test']\n",
        "counts = [train_counts, val_counts, test_counts]\n",
        "\n",
        "# Transpose counts to group by label\n",
        "benign_counts = [c[0] for c in counts]\n",
        "mal_counts = [c[1] for c in counts]\n",
        "\n",
        "x = np.arange(len(datasets))  # the label locations\n",
        "width = 0.35  # width of the bars\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, benign_counts, width, label='Benign')\n",
        "rects2 = ax.bar(x + width/2, mal_counts, width, label='Malicious')\n",
        "\n",
        "# Add labels\n",
        "ax.set_ylabel('Number of Samples')\n",
        "ax.set_title('Label Distribution Across Datasets')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(datasets)\n",
        "ax.legend()\n",
        "\n",
        "# Add value labels\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height}',\n",
        "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
        "                    xytext=(0, 3),  # vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wRbboPCjK-zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalise the datasets"
      ],
      "metadata": {
        "id": "uOj4HtZcH710"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# --- Normalize ---\n",
        "#scaler = RobustScaler() # Initialize the scaler\n",
        "scaler = StandardScaler() # Initialize the scaler\n",
        "\n",
        "# Apply the scaler\n",
        "X_train = scaler.fit_transform(X_train) # Standardise data features\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Print the shape of the scaled data to verify\n",
        "print(f\"Shape of X_train after scaling: {X_train.shape}\")\n",
        "print(f\"Shape of X_val after scaling: {X_val.shape}\")\n",
        "print(f\"Shape of X_test after scaling: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "j73x9iD6H-ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data reshaping for 1D CNN input"
      ],
      "metadata": {
        "id": "WBxWX3MftapB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Reshape the data for 1D CNN input\n",
        "# 1D CNN expects input shape: (samples, timesteps, features). timesteps = number of features, features = 1 (per timestep)\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "\n",
        "# --- Final Shape Confirmation ---\n",
        "print(\"Training input shape:\", X_train.shape)\n",
        "print(\"Validation input shape:\", X_val.shape)\n",
        "print(\"Test input shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "V0JKdNVLteDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1D CNN model"
      ],
      "metadata": {
        "id": "AaK_AOGeuPxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
        "\n",
        "#Step 3: Define the 1D CNN Model\n",
        "\n",
        "# Input shape\n",
        "input_shape = (X_train.shape[1], 1)  # input_shape = (time_steps, features) of the training data\n",
        "\n",
        "# Build model\n",
        "CNN_model = Sequential([\n",
        "    Input(shape=input_shape),\n",
        "    Conv1D(filters=32, kernel_size=3, activation='relu'),   # Learn 32 distinct patterns from the data -> try detect low-level, local patterns from raw features\n",
        "    BatchNormalization(),                                   # Normalizes the outputs of a the Conv1D layer before passing them to the MaxPool layer\n",
        "    MaxPooling1D(pool_size=2),                              # Reduce the dimensions of the data without affecting key features\n",
        "    Dropout(0.25),                                          # Prevent overfitting by forcing the model to generalize - it does this by randomly deactivating a fraction of neurons during training\n",
        "\n",
        "    Conv1D(filters=64, kernel_size=2, activation='relu'),   # Try detect more abstract/complex patterns from first Conv1D layer. Each filter will look at 2 consecutive input features at a time (kernel = 2) eg: [f1, f2], [f2, f3], ... #kernel_size was 3\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    Flatten(),                                                                        # Converts the output of the last Conv1D layer into a 1D vector for the next fully connected layers\n",
        "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),  # The layer learns 64 different feature detectors (kernels), each kernel learns a specific pattern\n",
        "                                                                                      # Apply L2 regularisation to prevent overfitting (common in the dense layer) #https://medium.com/@bhatadithya54764118/day-49-overfitting-and-underfitting-in-dl-regularization-techniques-8ded20baa3d6\n",
        "\n",
        "    Dropout(0.5),                                                                     # Randomly drop 50% of the network's neurons to further prevent overfitting\n",
        "    Dense(1, activation='sigmoid')                                                    # Final output layer( '1' = 1 neuron for binary classification, 'sigmoid' = decides if input is malicious (1) or benign (0))\n",
        "])\n",
        "\n",
        "# Compile 1DCNN_model\n",
        "CNN_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary\n",
        "# CNN_model.summary()"
      ],
      "metadata": {
        "id": "5z09AdYcuSTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying something new"
      ],
      "metadata": {
        "id": "mGJ7VIhg2Kpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
        "\n",
        "# #Step 3: Define the 1D CNN Model\n",
        "\n",
        "# # Input shape\n",
        "# input_shape = (X_train.shape[1], 1)  # input_shape = (time_steps, features) of the training data\n",
        "s\n",
        "# # Build model\n",
        "# CNN_model = Sequential([\n",
        "#     Input(shape=input_shape),\n",
        "#     Conv1D(filters=32, kernel_size=3, activation='relu'),   # Learn 32 distinct patterns from the data -> try detect low-level, local patterns from raw features\n",
        "#     BatchNormalization(),                                   # Normalizes the outputs of a the Conv1D layer before passing them to the MaxPool layer\n",
        "#     MaxPooling1D(pool_size=2),                              # Reduce the dimensions of the data without affecting key features\n",
        "\n",
        "#     Conv1D(filters=64, kernel_size=2, activation='relu'),   # Try detect more abstract/complex patterns from first Conv1D layer. Each filter will look at 2 consecutive input features at a time (kernel = 2) eg: [f1, f2], [f2, f3], ... #kernel_size was 3\n",
        "#     BatchNormalization(),\n",
        "#     MaxPooling1D(pool_size=2),\n",
        "\n",
        "#     Flatten(),                                                                        # Converts the output of the last Conv1D layer into a 1D vector for the next fully connected layers\n",
        "#     Dense(64, activation='relu'),  # The layer learns 64 different feature detectors (kernels), each kernel learns a specific pattern\n",
        "#                                                                                       # Apply L2 regularisation to prevent overfitting (common in the dense layer) #https://medium.com/@bhatadithya54764118/day-49-overfitting-and-underfitting-in-dl-regularization-techniques-8ded20baa3d6\n",
        "\n",
        "#     Dense(1, activation='sigmoid')                                                    # Final output layer( '1' = 1 neuron for binary classification, 'sigmoid' = decides if input is malicious (1) or benign (0))\n",
        "# ])\n",
        "\n",
        "# # Compile 1DCNN_model\n",
        "# CNN_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Summary\n",
        "# # CNN_model.summary()"
      ],
      "metadata": {
        "id": "P0arSG7f2OGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train 1D CNN\n",
        "\n",
        "- Major problems with class imbalance"
      ],
      "metadata": {
        "id": "h5WWCRHmusAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "\n",
        "# Train the Model\n",
        "start_time = timeit.default_timer()\n",
        "history = CNN_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "end_time = timeit.default_timer()\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "# Plot Accuracy and Loss\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "yGslPs6rurxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate 1D CNN Malware Detection Results"
      ],
      "metadata": {
        "id": "xR3T_G90vSvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# Get predictions of x_test dataset\n",
        "threshold = 0.5 #0.09 gave only 773 fn\n",
        "print(f\"threshold: {threshold}\")\n",
        "y_pred_probs = CNN_model.predict(X_test)\n",
        "y_pred = (y_pred_probs > threshold).astype(int).flatten()  # Boolean statmenent that flattens y_pred to 0 (False) if the CNN's prediction is bigger than the threshold\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Optional: print as table\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Detailed breakdown\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"\\nTrue Negatives (Benign correctly classified): {tn}\")\n",
        "print(f\"False Positives (Benign misclassified as malware): {fp}\")\n",
        "print(f\"False Negatives (Malware missed): {fn}\")\n",
        "print(f\"True Positives (Malware correctly identified): {tp}\")\n",
        "\n",
        "# Accuracy scores\n",
        "print(\"Accuracy:\")\n",
        "print(\"sklearn Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "test_loss, test_acc = CNN_model.evaluate(X_test, y_test, verbose=0)# Evaluate model with test set\n",
        "print(f\"model.evaluate Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Confusion Matrix Accuracy: {(tp + tn) / (tp + tn + fp + fn)}\")\n",
        "print(f\"{(tp/(tp+fn))*100:.4f}% of malware identified\")\n",
        "print(f\"{(fp/(fp+tn))*100:.4f} of benign traffic incorrectly identified\")\n",
        "\n",
        "# Classification report (accuracy, precision, recall, F1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Benign\", \"Malicious\"]))"
      ],
      "metadata": {
        "id": "G1chgEQVvU-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot of CNN's predictions vs. actual labels"
      ],
      "metadata": {
        "id": "JwIx117UuZXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- Plot CNN Prediction Probabilities ---\n",
        "# Map actual labels to colors: red = malicious (1), blue = benign (0)\n",
        "colors = np.where(y_test == 1, 'red', 'blue')\n",
        "\n",
        "# X-axis = sample index\n",
        "x_vals = np.arange(len(y_pred_probs))\n",
        "\n",
        "# Plot predictions with color by actual label\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(x_vals, y_pred_probs, c=colors, alpha=0.3, s=5)\n",
        "plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold = 0.2')\n",
        "\n",
        "# Labels and styling\n",
        "plt.ylim([0, 1])\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Predicted Probability\")\n",
        "plt.title(\"CNN Malware Prediction Probabilities Colored by Actual Label\")\n",
        "custom_lines = [\n",
        "    plt.Line2D([0], [0], marker='o', color='w', label='Benign (y=0)', markerfacecolor='blue', markersize=6),\n",
        "    plt.Line2D([0], [0], marker='o', color='w', label='Malicious (y=1)', markerfacecolor='red', markersize=6)\n",
        "]\n",
        "plt.legend(handles=custom_lines)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fjo0-Sc-ueHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1D CNN Quant model"
      ],
      "metadata": {
        "id": "L3gfDVIU2grG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# --- STEP 1: Balance the sample set to 40% benign, 60% malware ---\n",
        "def balance_sample_data(X_train, y_train, benign_ratio=0.4):\n",
        "    benign_X = X_train[y_train == 0]\n",
        "    malware_X = X_train[y_train == 1]\n",
        "\n",
        "    total = min(len(benign_X) + len(benign_X), 100000)\n",
        "    benign_n = int(benign_ratio * total)\n",
        "    malware_n = total - benign_n\n",
        "\n",
        "    benign_X_sampled = benign_X[np.random.choice(len(benign_X), benign_n)]\n",
        "    malware_X_sampled = malware_X[np.random.choice(len(malware_X), malware_n)] #ISSUE?\n",
        "\n",
        "    X_bal = np.vstack((benign_X_sampled, malware_X_sampled))\n",
        "    y_bal = np.array([0]*benign_n + [1]*malware_n)\n",
        "\n",
        "    return shuffle(X_bal, y_bal, random_state=42)\n",
        "\n",
        "# Apply balancing to training data\n",
        "X_sample, y_sample = balance_sample_data(X_train, y_train, benign_ratio=0.4)\n",
        "\n",
        "# Ensure 1D shape (samples, timesteps, 1)\n",
        "X_sample = X_sample.reshape((X_sample.shape[0], X_sample.shape[1], 1))\n",
        "\n",
        "# --- STEP 2: Define representative dataset for quantization ---\n",
        "def representative_dataset():\n",
        "    for i in range(1000):\n",
        "        index = np.random.randint(0, X_sample.shape[0]) #randomise choices from X_sample data to get a range of diverse samples (not just the first 1000)\n",
        "        yield [X_sample[index:index+1].astype(np.float32)]\n",
        "\n",
        "# --- STEP 3: Quantization function ---\n",
        "def converter_quantization_model(model, model_name):\n",
        "    os.makedirs('./models', exist_ok=True)\n",
        "\n",
        "    # Save float32 optimized model\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_types = [tf.float32]\n",
        "    converter.representative_dataset = representative_dataset\n",
        "    tflite_model_float32 = converter.convert()\n",
        "    with open(model_name + '_quant_float32.tflite', 'wb') as f:\n",
        "        f.write(tflite_model_float32)\n",
        "\n",
        "    # Save int8 quantized model\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_types = [tf.int8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "    converter.representative_dataset = representative_dataset\n",
        "    tflite_model_int8 = converter.convert()\n",
        "    with open(model_name + '_quant_int8.tflite', 'wb') as f:\n",
        "        f.write(tflite_model_int8)\n",
        "\n",
        "# --- STEP 4: Apply conversion ---\n",
        "converter_quantization_model(CNN_model, './models/1Dcnn')\n"
      ],
      "metadata": {
        "id": "PjeYR1Sf2j9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate int8 1D CNN"
      ],
      "metadata": {
        "id": "9Em5y-Cl26vO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.lite.python.interpreter import Interpreter\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "def evaluate_tflite_model(tflite_path, X_test, y_test):\n",
        "    #Load and Prepare TFLite Interpreter\n",
        "    interpreter = Interpreter(model_path=tflite_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    #Fetch input & output Tensor Details from the TensorFlow Lite Interpreter after the model is loaded and memory for its tensors is allocated.\n",
        "    input_details = interpreter.get_input_details() #Input tensor: shape, dtype, index.\n",
        "    output_details = interpreter.get_output_details() #Output tensor: shape, dtype, index, quantisation.\n",
        "\n",
        "    #Store the tensor indices and data types for input/output\n",
        "    input_index = input_details[0]['index']\n",
        "    output_index = output_details[0]['index']\n",
        "    input_dtype = input_details[0]['dtype']\n",
        "    output_dtype = output_details[0]['dtype']\n",
        "\n",
        "    # Calculate Quantization Threshold\n",
        "    output_scale, output_zero_point = output_details[0]['quantization']#Get quantization parameters for the output\n",
        "    if output_dtype == np.int8:\n",
        "        quantized_threshold = int(round(0.5 / output_scale + output_zero_point)) #Computes the int8 value that corresponds to 0.5 — the usual binary classification threshold for float.\n",
        "\n",
        "    #Initializes empty lists to store predictions and timing\n",
        "    predictions = []\n",
        "    processing_times = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        #input_data = np.expand_dims(X_test[i], axis=0).astype(input_dtype)#Prepare Input Tensor and Run Inference\n",
        "        input_data = np.expand_dims(X_test[i], axis=0)\n",
        "\n",
        "        if input_dtype == np.int8:\n",
        "            input_scale, input_zero_point = input_details[0]['quantization']\n",
        "            input_data = (input_data / input_scale + input_zero_point).astype(np.int8)\n",
        "        else:\n",
        "            input_data = input_data.astype(input_dtype)\n",
        "\n",
        "        start_time = time.time()\n",
        "        interpreter.set_tensor(input_index, input_data) #Loads input into the interpreter.\n",
        "        interpreter.invoke() #Execute the model\n",
        "        output = interpreter.get_tensor(output_index) #Gets the output tensor.\n",
        "        processing_times.append(time.time() - start_time)\n",
        "\n",
        "        # Handle prediction thresholding for float and int8 models\n",
        "        if output_dtype == np.float32:\n",
        "            pred = (output[0][0] > 0.5).astype(int)\n",
        "        elif output_dtype == np.int8:\n",
        "            pred = (output[0][0] > quantized_threshold).astype(int) #If int8, compare against precomputed quantized threshold\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported output dtype: {output_dtype}\")\n",
        "\n",
        "        predictions.append(pred)\n",
        "\n",
        "    #Evaluate Accuracy and Confusion Matrix\n",
        "    acc = accuracy_score(y_test, predictions)\n",
        "    cm = confusion_matrix(y_test, predictions)\n",
        "\n",
        "    print(f\"{tflite_path} Accuracy: {acc:.4f}\")\n",
        "    print(f\"Avg inference time: {np.mean(processing_times)*1000:.2f} ms/sample\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(f\"{(tp/(tp+fn))*100}% of malware idenetified\")\n",
        "\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        print(f\"\\nTrue Negatives (Benign correctly classified): {tn}\")\n",
        "        print(f\"False Positives (Benign misclassified as malware): {fp}\")\n",
        "        print(f\"False Negatives (Malware missed): {fn}\")\n",
        "        print(f\"True Positives (Malware correctly identified): {tp}\")\n",
        "    else:\n",
        "        print(\"\\nWarning: Confusion matrix is not 2x2 — check labels.\")\n",
        "\n",
        "# Execute evaluation function:\n",
        "evaluate_tflite_model(\"./models/1Dcnn_quant_float32.tflite\", X_test, y_test)\n",
        "evaluate_tflite_model(\"./models/1Dcnn_quant_int8.tflite\", X_test, y_test)\n"
      ],
      "metadata": {
        "id": "heu0aEfQ2_hA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}