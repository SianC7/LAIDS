{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SianC7/LAIDS/blob/main/Final_1D_CNN_Baseline_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXZ5uI8bDP3t"
      },
      "source": [
        "#CICIDS2017 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOAMv_tr9ew1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e815dd51-f48e-43f5-d875-1d6e2a199dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3zsgHCrDVBI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Data Collection ---\n",
        "\n",
        "# Set pandas display options for wide output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "# Get Data file path\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/cicids2017_cleaned.csv'\n",
        "#file_path = \"/content/drive/MyDrive/Honours Project/Datasets/ADASYN_CICIDS2017_Dataset.csv\" # for Claire\n",
        "# file_path = \"/content/drive/MyDrive/Honours Project/Datasets/CICIDS2017 ADASYN Dataset/ADASYN_CICIDS2017_Dataset.csv\" #For Chris\n",
        "cicids2017_df = pd.read_csv(file_path, sep=\",\", comment=\"#\", header=0)\n",
        "cicids2017_df.columns = cicids2017_df.columns.str.strip()  # Strip whitespace from column names\n",
        "\n",
        "\n",
        "print(\"\\nInitial samples:\")\n",
        "print(f\"cicids2017_df shape: {cicids2017_df.shape}\")\n",
        "# print(cicids2017_df.head().to_string())\n",
        "# print(cicids2017_df.info())\n",
        "\n",
        "# Print unique values and their counts for 'Attack Type'\n",
        "print(\"\\nAttack Type Distribution:\")\n",
        "print(cicids2017_df['Attack Type'].value_counts())\n",
        "\n",
        "# --- Label Encoding ---\n",
        "\n",
        "# Get unique attack types\n",
        "attack_types = cicids2017_df['Attack Type'].unique()\n",
        "\n",
        "# Create a mapping from attack type to integer label\n",
        "attack_type_map = {'Normal Traffic': 0, 'Port Scanning': 1, 'Web Attacks': 2, 'Brute Force': 3, 'DDoS': 4, 'Bots': 5, 'DoS': 6} # Use the specified mapping\n",
        "\n",
        "# Apply label encoding\n",
        "cicids2017_df['Attack Type'] = cicids2017_df['Attack Type'].map(attack_type_map)\n",
        "\n",
        "print(\"\\nLabel Encoding Mapping:\")\n",
        "print(attack_type_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_-3p8agD5ps"
      },
      "source": [
        "# Train/val/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0CyZlVTD-QL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# --- Train/val/test split ---\n",
        "# Split label from datafram\n",
        "X = cicids2017_df.drop('Attack Type', axis=1)\n",
        "y = cicids2017_df['Attack Type']\n",
        "\n",
        "# Split Data\n",
        "X_temp, X_test, y_temp, y_test= train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify = y_temp)\n",
        "\n",
        "# Shuffle the data\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
        "X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
        "X_test, y_test = shuffle(X_test, y_test, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBlQLyboK9hQ"
      },
      "source": [
        "Visualisation of data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRbboPCjK-zB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"Input Shapes -> Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# Get the counts for each attack type in each dataset\n",
        "train_counts = np.bincount(y_train)\n",
        "val_counts = np.bincount(y_val)\n",
        "test_counts = np.bincount(y_test)\n",
        "\n",
        "# Get the number of unique attack types (based on the maximum index found)\n",
        "num_attack_types = max(len(train_counts), len(val_counts), len(test_counts))\n",
        "\n",
        "# Pad counts with zeros if some attack types are missing in a dataset split\n",
        "train_counts = np.pad(train_counts, (0, num_attack_types - len(train_counts)), 'constant')\n",
        "val_counts = np.pad(val_counts, (0, num_attack_types - len(val_counts)), 'constant')\n",
        "test_counts = np.pad(test_counts, (0, num_attack_types - len(test_counts)), 'constant')\n",
        "\n",
        "\n",
        "print(f\"Labels distribution -> Train: {train_counts}, Val: {val_counts}, Test: {test_counts}\")\n",
        "\n",
        "# Data for plotting\n",
        "datasets = ['Train', 'Validation', 'Test']\n",
        "all_counts = np.array([train_counts, val_counts, test_counts])\n",
        "\n",
        "# Get the original attack type names from the mapping\n",
        "# Need to reverse the mapping to get names from labels\n",
        "reverse_attack_type_map = {v: k for k, v in attack_type_map.items()}\n",
        "labels = [reverse_attack_type_map.get(i, f'Unknown {i}') for i in range(num_attack_types)]\n",
        "\n",
        "x = np.arange(len(labels))  # the label locations for attack types\n",
        "width = 0.25  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6)) # Increase figure size\n",
        "\n",
        "rects1 = ax.bar(x - width, all_counts[0], width, label='Train')\n",
        "rects2 = ax.bar(x, all_counts[1], width, label='Validation')\n",
        "rects3 = ax.bar(x + width, all_counts[2], width, label='Test')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Number of Samples')\n",
        "ax.set_title('Attack Type Distribution Across Datasets')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, rotation=45, ha=\"right\") # Rotate labels for better readability\n",
        "ax.legend()\n",
        "\n",
        "# Add value labels (optional, can make the plot cluttered with many categories)\n",
        "def autolabel(rects):\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "autolabel(rects3)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOj4HtZcH710"
      },
      "source": [
        "#Normalise the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j73x9iD6H-ij"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# --- Normalize ---\n",
        "scaler = StandardScaler() # Initialize the scaler\n",
        "\n",
        "# Apply the scaler\n",
        "X_train = scaler.fit_transform(X_train) # Standardise data features\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Print the shape of the scaled data to verify\n",
        "print(f\"Shape of X_train after scaling: {X_train.shape}\")\n",
        "print(f\"Shape of X_val after scaling: {X_val.shape}\")\n",
        "print(f\"Shape of X_test after scaling: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBxWX3MftapB"
      },
      "source": [
        "# Data reshaping for 1D CNN input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0JKdNVLteDk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Reshape the data for 1D CNN input\n",
        "# 1D CNN expects input shape: (samples, timesteps, features). timesteps = number of features, features = 1 (per timestep)\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "\n",
        "# --- Final Shape Confirmation ---\n",
        "print(\"Training input shape:\", X_train.shape)\n",
        "print(\"Validation input shape:\", X_val.shape)\n",
        "print(\"Test input shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Us26JycbD_g"
      },
      "source": [
        "# Perform Hyperparameter Tuning (Bayesian Optimisation) of 2 layer CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjb1W2GwlKkn"
      },
      "outputs": [],
      "source": [
        "# # Install tuner\n",
        "# !pip install keras-tuner --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55_BSRtnlNrC"
      },
      "outputs": [],
      "source": [
        "# # Setup\n",
        "# import keras_tuner as kt\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "# from tensorflow.keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZSTewTvbJLy"
      },
      "outputs": [],
      "source": [
        "# def build_base_model(hp):\n",
        "#     model = Sequential()\n",
        "\n",
        "#     # First Conv1D block\n",
        "#     model.add(Conv1D(\n",
        "#         filters=hp.Choice('conv1_filters', values=[8, 16, 32, 64, 128]),\n",
        "#         kernel_size=hp.Choice('conv1_kernel_size', values=[2, 3, 5]),\n",
        "#         activation='relu',\n",
        "#         input_shape=(52, 1) # Define input shape here\n",
        "#     ))\n",
        "\n",
        "#     model.add(BatchNormalization())\n",
        "#     model.add(MaxPooling1D(2))\n",
        "#     model.add(Dropout(hp.Choice('dropout1_rate', [0.0,0.2, 0.25, 0.3, 0.5])))\n",
        "\n",
        "#     # Second Conv1D block\n",
        "#     model.add(Conv1D(\n",
        "#         filters=hp.Choice('conv2_filters', values=[8,16, 32, 64, 128]),\n",
        "#         kernel_size=hp.Choice('conv2_kernel_size', values=[2, 3, 5]),\n",
        "#         activation='relu'\n",
        "#     ))\n",
        "#     model.add(BatchNormalization())\n",
        "#     model.add(MaxPooling1D(2))\n",
        "\n",
        "#     # Flatten previous layers\n",
        "#     model.add(Flatten())\n",
        "\n",
        "#     # Dense layer with L2 regularization\n",
        "#     model.add(Dense(\n",
        "#         hp.Choice('dense_units', [8, 12, 24, 64]),\n",
        "#         activation='relu',\n",
        "#         kernel_regularizer=regularizers.l2(hp.Choice('dense_L2', [0.0, 0.001, 0.0001, 0.01]))\n",
        "#     ))\n",
        "\n",
        "#     # Dropout layer\n",
        "#     model.add(Dropout(hp.Choice('dropout2_rate', [0.0, 0.2, 0.25, 0.3, 0.5])))\n",
        "\n",
        "#     # Dense layer for multiclass classification with softmax activation\n",
        "#     model.add(Dense(7, activation='softmax')) # Assuming 7 classes based on the value_counts output\n",
        "\n",
        "#     # Compile\n",
        "#     model.compile(\n",
        "#         optimizer= tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', [0.001, 0.005, 0.01])), #'adam', #add optimiser choices?\n",
        "#         loss='sparse_categorical_crossentropy', # Use sparse_categorical_crossentropy for integer labels\n",
        "#         metrics=['accuracy']\n",
        "#     )\n",
        "#     return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3soZK0wcGNE"
      },
      "outputs": [],
      "source": [
        "# # --- Create tuner ---\n",
        "# tuner = kt.BayesianOptimization(\n",
        "#     build_base_model,\n",
        "#     objective= 'val_accuracy',#kt.Objective('val_f1_score', direction='max'),# Find the hyperparameters that give the highest possible F1 score on the validation set\n",
        "#     max_trials=10,\n",
        "#     directory='bayesian_tuning',\n",
        "#     project_name='baseline_cnn'\n",
        "# )\n",
        "\n",
        "# # --- Implement callback ---\n",
        "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', # Specify monitor='val_loss' to track the validation loss.\n",
        "#                                patience=3, #the number of epochs to wait for an improvement\n",
        "#                                restore_best_weights=True) # Revert the model to its state where it outputted the lowest validation loss\n",
        "\n",
        "# # --- Start tuning ---\n",
        "# tuner.search(X_train, y_train,\n",
        "#              epochs=10,\n",
        "#              batch_size=64,\n",
        "#              validation_data=(X_val, y_val),\n",
        "#              callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9oLkyd3cP2y"
      },
      "outputs": [],
      "source": [
        "# # --- Display Best Variables ---\n",
        "# best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "# print(\"Best Hyperparameters:\")\n",
        "# for param in best_hps.values:\n",
        "#     print(f\"{param}: {best_hps.get(param)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV4RKvVaciZE"
      },
      "outputs": [],
      "source": [
        "# # --- Visulise tunning results ---\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# trials = tuner.oracle.get_best_trials(num_trials=20)\n",
        "# val_accuracies = [t.metrics.get_last_value('val_accuracy') for t in trials]\n",
        "\n",
        "# plt.plot(val_accuracies, marker='o')\n",
        "# plt.title('Validation Accuracy per Trial')\n",
        "# plt.xlabel('Trial')\n",
        "# plt.ylabel('Val Accuracy')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaK_AOGeuPxj"
      },
      "source": [
        "#1D CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNlv3XoEcEHs"
      },
      "outputs": [],
      "source": [
        "#%pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
        "\n",
        "# Define the 1D CNN Model\n",
        "# Best Hyperparameters:\n",
        "# conv1_filters: 32\n",
        "# conv1_kernel_size: 2\n",
        "# dropout1_rate: 0.0\n",
        "# conv2_filters: 16\n",
        "# conv2_kernel_size: 3\n",
        "# dense_units: 64\n",
        "# dense_L2: 0.0\n",
        "# dropout2_rate: 0.5\n",
        "# learning_rate: 0.005\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (X_train.shape[1], 1)  # (timesteps, features)\n",
        "num_classes = len(attack_type_map) # Get the number of unique attack types for the output layer\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Input(shape=input_shape),\n",
        "\n",
        "    Conv1D(filters=32, kernel_size=2, activation='relu'), # Detect patterns in the network traffic data\n",
        "    BatchNormalization(), # Normalizes the outputs of a the Conv1D layer before passing them to the MaxPool layer\n",
        "    MaxPooling1D(pool_size=2), # Reduce the dimensions of the data without affecting key features\n",
        "    Dropout(0.0), # Prevent overfitting by forcing the model to generalize - it does this by randomly deactivating a fraction of neurons during training\n",
        "\n",
        "    Conv1D(filters=16, kernel_size=3, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    Flatten(), # Converts the output of the last Conv1D layer into a 1D vector for the fully connected layers\n",
        "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0)), # Apply L2 regularisation to prevent overfitting (common in the dense layer) #https://medium.com/@bhatadithya54764118/day-49-overfitting-and-underfitting-in-dl-regularization-techniques-8ded20baa3d6\n",
        "    Dropout(0.5), # Randomly drop 50% of the network's neurons to further prevent overfitting\n",
        "    Dense(num_classes, activation='softmax')  # Final output layer for multiclass classification\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile( optimizer = tf.keras.optimizers.Adam(learning_rate=0.005),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy']) # Loss function measures how well the model’s predictions match true labels #'adam'\n",
        "\n",
        "# Summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5WWCRHmusAX"
      },
      "source": [
        "#Train 1D CNN\n",
        "\n",
        "- Major problems with class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGslPs6rurxy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# --- Early-stopping --- #TESTING\n",
        "# https://medium.com/@piyushkashyap045/early-stopping-in-deep-learning-a-simple-guide-to-prevent-overfitting-1073f56b493e\n",
        "# Early-stopping is a regularisation technique that prevents overfitting by stopping the training process when the model’s performance on the validation dataset starts degrading\n",
        "# Stopping early reduces training time and computational costs\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', # Specify monitor='val_loss' to track the validation loss.\n",
        "                               patience=3, #the number of epochs to wait for an improvement\n",
        "                               restore_best_weights=True) # Revert the model to its state where it outputted the lowest validation loss\n",
        "\n",
        "# Train the model with early stopping\n",
        "start_time = timeit.default_timer()\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "end_time = timeit.default_timer()\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "# Plot Accuracy and Loss\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-fold Cross Validation 1\n",
        "https://media.datacamp.com/legacy/v1718738336/image_0bb32b40f1.jpg\n"
      ],
      "metadata": {
        "id": "LwYuUdzr-DHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stratified K-fold Cross Validation"
      ],
      "metadata": {
        "id": "0FKFLwYuEzTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# from statistics import mean, stdev\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
        "# from tensorflow.keras.callbacks import EarlyStopping\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# X = cicids2017_df.drop('Attack Type', axis=1)\n",
        "# y = cicids2017_df['Attack Type']\n",
        "\n",
        "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "# accuracy_scores = []\n",
        "\n",
        "# for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "#     print(f\"\\n--- Fold {fold+1} ---\")\n",
        "\n",
        "#     # Split data\n",
        "#     x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "#     y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "\n",
        "#     #Normalise\n",
        "#     x_train_fold = scaler.fit_transform(x_train_fold)\n",
        "#     x_test_fold = scaler.transform(x_test_fold)\n",
        "\n",
        "#     #Shape for CNN Input\n",
        "#     x_train_fold = x_train_fold.reshape((x_train_fold.shape[0], x_train_fold.shape[1], 1))\n",
        "#     x_test_fold = x_test_fold.reshape((x_test_fold.shape[0], x_test_fold.shape[1], 1))\n",
        "\n",
        "#     # Define model inside loop\n",
        "#     model = Sequential([\n",
        "#         Input(shape=(x_train_fold.shape[1], 1)),\n",
        "#         Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         MaxPooling1D(pool_size=2),\n",
        "#         Dropout(0.0),\n",
        "#         Conv1D(filters=16, kernel_size=3, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         MaxPooling1D(pool_size=2),\n",
        "#         Flatten(),\n",
        "#         Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0)),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(len(attack_type_map), activation='softmax')\n",
        "#     ])\n",
        "\n",
        "#     model.compile(\n",
        "#         optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
        "#         loss='sparse_categorical_crossentropy',\n",
        "#         metrics=['accuracy']\n",
        "#     )\n",
        "\n",
        "#     # Early stopping\n",
        "#     early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "#     # Train\n",
        "#     history = model.fit(\n",
        "#         x_train_fold, y_train_fold,\n",
        "#         epochs=10,\n",
        "#         batch_size=64,\n",
        "#         validation_data=(x_test_fold, y_test_fold),\n",
        "#         callbacks=[early_stopping],\n",
        "#         verbose=0\n",
        "#     )\n",
        "\n",
        "#     # Evaluate\n",
        "#     loss, acc = model.evaluate(x_test_fold, y_test_fold, verbose=0)\n",
        "#     print(f\"Fold {fold+1} Accuracy: {acc:.4f}\")\n",
        "#     accuracy_scores.append(acc)\n",
        "\n",
        "# # Final results\n",
        "# print('\\nList of accuracy scores:', accuracy_scores)\n",
        "# print('Maximum Accuracy: {:.2f}%'.format(max(accuracy_scores) * 100))\n",
        "# print('Minimum Accuracy: {:.2f}%'.format(min(accuracy_scores) * 100))\n",
        "# print('Mean Accuracy: {:.2f}%'.format(mean(accuracy_scores) * 100))\n",
        "# print('Standard Deviation: {:.4f}'.format(stdev(accuracy_scores)))"
      ],
      "metadata": {
        "id": "zu01l-VxE2Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Fold Cross Validation 2"
      ],
      "metadata": {
        "id": "lnrzUnebhNkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "cuBFA-o2hQww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from statistics import mean, stdev\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
        "# from tensorflow.keras.callbacks import EarlyStopping\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "\n"
      ],
      "metadata": {
        "id": "-NQUBfwAhMf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model creation"
      ],
      "metadata": {
        "id": "vEAjlWgihR3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_cnn_model(input_shape, num_classes):\n",
        "#     model = Sequential([\n",
        "#         Input(shape=input_shape),\n",
        "#         Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         MaxPooling1D(pool_size=2),\n",
        "#         Dropout(0.0),\n",
        "#         Conv1D(filters=16, kernel_size=3, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         MaxPooling1D(pool_size=2),\n",
        "#         Flatten(),\n",
        "#         Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0)),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(num_classes, activation='softmax')\n",
        "#     ])\n",
        "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
        "#                   loss='sparse_categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "#     return model"
      ],
      "metadata": {
        "id": "bzSNCwlmhUg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross validation loop"
      ],
      "metadata": {
        "id": "RIU7fYfGhbnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# import numpy as np\n",
        "\n",
        "# X = cicids2017_df.drop('Attack Type', axis=1).values\n",
        "# y = cicids2017_df['Attack Type'].values\n",
        "\n",
        "# # Calculate weights from the original label vector\n",
        "# class_weights = compute_class_weight(\n",
        "#     class_weight='balanced',\n",
        "#     classes=np.unique(y),\n",
        "#     y=y\n",
        "# )\n",
        "# class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# fold = 1\n",
        "# results = []\n",
        "\n",
        "# for train_index, test_index in skf.split(X, y):\n",
        "#     print(f\"\\nFold {fold}\")\n",
        "\n",
        "#     X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
        "#     y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "#     # Scaling\n",
        "#     scaler = StandardScaler()\n",
        "#     X_train_scaled = scaler.fit_transform(X_train_fold)\n",
        "#     X_test_scaled = scaler.transform(X_test_fold)\n",
        "\n",
        "#     # Reshape for 1D CNN\n",
        "#     X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "#     X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "#     # Build model\n",
        "#     model = create_cnn_model(input_shape=(X_train_scaled.shape[1], 1), num_classes=len(attack_type_map))\n",
        "\n",
        "#     # Early stopping\n",
        "#     early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "#     # Train\n",
        "#     history = model.fit(X_train_scaled, y_train_fold,\n",
        "#                         validation_split=0.1,\n",
        "#                         epochs=10,\n",
        "#                         batch_size=64,\n",
        "#                         callbacks=[early_stopping],\n",
        "#                         class_weight=class_weight_dict,\n",
        "#                         verbose=0)\n",
        "\n",
        "#     # Predict & evaluate\n",
        "#     y_pred_probs = model.predict(X_test_scaled)\n",
        "#     y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "#     acc = accuracy_score(y_test_fold, y_pred)\n",
        "#     print(f\"Fold {fold} Accuracy: {acc:.4f}\")\n",
        "#     missing_preds = np.setdiff1d(np.unique(y_test_fold), np.unique(y_pred))\n",
        "#     print(f\"Classes not predicted in Fold {fold}: {missing_preds}\")\n",
        "#     results.append({\n",
        "#         'fold': fold,\n",
        "#         'accuracy': acc,\n",
        "#         'report': classification_report(y_test_fold, y_pred, output_dict=True),\n",
        "#         'confusion_matrix': confusion_matrix(y_test_fold, y_pred)\n",
        "#     })\n",
        "\n",
        "#     fold += 1"
      ],
      "metadata": {
        "id": "zmqGlI3Phd-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarise results"
      ],
      "metadata": {
        "id": "N6nPvuuAi_Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import seaborn as sns\n",
        "\n",
        "# # Average accuracy\n",
        "# avg_acc = np.mean([r['accuracy'] for r in results])\n",
        "# print(f\"\\nAverage Accuracy over 5 folds: {avg_acc:.4f}\")\n",
        "\n",
        "# # Summary of results\n",
        "# for r in results:\n",
        "#     fold = r['fold']\n",
        "#     acc = r['accuracy']\n",
        "#     f1 = r['report']['macro avg']['f1-score']\n",
        "#     print(f\"Fold {fold}: Accuracy = {acc:.4f}, Macro F1 = {f1:.4f}\")\n",
        "\n",
        "# # Print missing classes\n",
        "# reverse_attack_type_map = {v: k for k, v in attack_type_map.items()}\n",
        "\n",
        "# for r in results:\n",
        "#     fold = r['fold']\n",
        "#     cm = r['confusion_matrix']\n",
        "#     pred_totals = cm.sum(axis=0)\n",
        "\n",
        "#     missing_classes = [i for i, val in enumerate(pred_totals) if val == 0]\n",
        "#     missing_class_names = [reverse_attack_type_map[i] for i in missing_classes]\n",
        "\n",
        "#     if missing_class_names:\n",
        "#         print(f\"Fold {fold} missed predicting classes: {missing_class_names}\")\n",
        "\n",
        "# for r in results:\n",
        "#     fold = r['fold']\n",
        "#     cm = r['confusion_matrix']\n",
        "\n",
        "#     # Plot the heatmap\n",
        "#     plt.figure(figsize=(8, 6))\n",
        "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "#                 xticklabels=[reverse_attack_type_map[i] for i in range(cm.shape[0])],\n",
        "#                 yticklabels=[reverse_attack_type_map[i] for i in range(cm.shape[0])])\n",
        "#     plt.title(f\"Fold {fold} Confusion Matrix\")\n",
        "#     plt.xlabel(\"Predicted\")\n",
        "#     plt.ylabel(\"True\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # Plot accuracy across all folds\n",
        "# folds = [r['fold'] for r in results]\n",
        "# accuracies = [r['accuracy'] for r in results]\n",
        "\n",
        "# plt.plot(folds, accuracies, marker='o')\n",
        "# plt.title(\"Accuracy Across Folds\")\n",
        "# plt.xlabel(\"Fold\")\n",
        "# plt.ylabel(\"Accuracy\")\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "yHSPuDpqjAS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR3T_G90vSvu"
      },
      "source": [
        "# Evaluate 1D CNN Malware Detection Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1chgEQVvU-s"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import seaborn as sns # Import seaborn for heatmap plotting\n",
        "import numpy as np # Import numpy\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "import tensorflow as tf # Import tensorflow\n",
        "\n",
        "# Get models predictions of x_test dataset\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "# For multiclass classification, the prediction is the class with the highest probability\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# --- Confusion matrix ---\n",
        "# For multiclass, confusion_matrix directly handles the true and predicted labels\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "\n",
        "# --- Heatmap ---\n",
        "# Get the original attack type names from the mapping\n",
        "reverse_attack_type_map = {v: k for k, v in attack_type_map.items()}\n",
        "labels = [reverse_attack_type_map.get(i, f'Unknown {i}') for i in range(cm.shape[0])] # Use matrix shape for label count\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6)) # Adjust figure size\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Multiclass Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Accuracy scores\n",
        "print(\"\\nAccuracy:\")\n",
        "print(\"sklearn Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# --- Debugging step: Print the shape of X_test ---\n",
        "print(f\"Shape of X_test before model.evaluate: {X_test.shape}\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)# Evaluate model with test set\n",
        "print(f\"model.evaluate Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Classification report (accuracy, precision, recall, F1)\n",
        "print(\"\\nClassification Report:\")\n",
        "# target_names should be the actual class names\n",
        "report = classification_report(y_test, y_pred, target_names=labels, output_dict=True)\n",
        "print(classification_report(y_test, y_pred, target_names=labels))\n",
        "\n",
        "# --- Custom Metrics for Malware vs Benign (Normal Traffic) ---\n",
        "normal_traffic_label = attack_type_map.get('Normal Traffic', None)\n",
        "\n",
        "if normal_traffic_label is not None:\n",
        "    # True Positives for Malware = sum of diagonals excluding 'Normal Traffic'\n",
        "    malware_identified_count = np.sum(np.diag(cm)) - cm[normal_traffic_label, normal_traffic_label]\n",
        "\n",
        "    # Total Malware = sum of all non-'Normal Traffic' samples\n",
        "    total_malware_count = np.sum(cm) - np.sum(cm[normal_traffic_label, :])\n",
        "\n",
        "    # % of Malware Identified\n",
        "    percentage_malware_identified = (malware_identified_count / total_malware_count) * 100 if total_malware_count > 0 else 0\n",
        "\n",
        "    # False Positives = Non-'Normal' samples predicted as 'Normal'\n",
        "    benign_not_identified_count = np.sum(cm[normal_traffic_label, :]) - cm[normal_traffic_label, normal_traffic_label]\n",
        "    total_benign_count = np.sum(cm[normal_traffic_label, :])\n",
        "\n",
        "    # % of Benign Traffic Misclassified\n",
        "    percentage_benign_not_identified = (benign_not_identified_count / total_benign_count) * 100 if total_benign_count > 0 else 0\n",
        "\n",
        "    print(f\"Total Malware Samples: {total_malware_count}\")\n",
        "    print(f\"Malware Identified (True Positives): {malware_identified_count}\")\n",
        "    print(f\"Percentage of Malware Identified: {percentage_malware_identified:.2f}%\")\n",
        "    print(f\"Total Benign Samples: {total_benign_count}\")\n",
        "    print(f\"Benign Misclassified as Malware (False Positives): {benign_not_identified_count}\")\n",
        "    print(f\"Percentage of Benign Misclassified: {percentage_benign_not_identified:.2f}%\")\n",
        "\n",
        "    # --- Calculate and print TP, TN, FP, FN for Malware vs Benign ---\n",
        "    # For binary classification (Malware vs Benign):\n",
        "    # TP: Malware correctly predicted as Malware (malware_identified_count)\n",
        "    # TN: Benign correctly predicted as Benign (cm[normal_traffic_label, normal_traffic_label])\n",
        "    # FP: Benign incorrectly predicted as Malware (benign_not_identified_count)\n",
        "    # FN: Malware incorrectly predicted as Benign (Total Malware Samples - Malware Identified)\n",
        "\n",
        "    tp = malware_identified_count\n",
        "    tn = cm[normal_traffic_label, normal_traffic_label]\n",
        "    fp = benign_not_identified_count\n",
        "    fn = total_malware_count - malware_identified_count\n",
        "\n",
        "    print(\"\\nTP, TN, FP, FN for Malware vs Benign:\")\n",
        "    print(f\"True Positives (TP): {tp}\")\n",
        "    print(f\"True Negatives (TN): {tn}\")\n",
        "    print(f\"False Positives (FP): {fp}\")\n",
        "    print(f\"False Negatives (FN): {fn}\")\n",
        "\n",
        "# --- Percentage of each malware class correctly identified ---\n",
        "    print(\"\\nPercentage of each Malware Class Correctly Identified:\")\n",
        "    for i, label in enumerate(labels):\n",
        "        if i != normal_traffic_label: # Exclude 'Normal Traffic'\n",
        "            correctly_identified = cm[i, i]\n",
        "            total_in_class = np.sum(cm[i, :])\n",
        "            percentage_identified = (correctly_identified / total_in_class) * 100 if total_in_class > 0 else 0\n",
        "            print(f\"{label}: {percentage_identified:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save best baseline model"
      ],
      "metadata": {
        "id": "j87wXnsTxE6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# import os\n",
        "\n",
        "# # Define the directory path\n",
        "# save_dir = '/content/drive/MyDrive/Colab Notebooks/Honours Project'\n",
        "\n",
        "# # Create the directory if it doesn't exist\n",
        "# os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# # Save the trained model\n",
        "# model_path = os.path.join(save_dir, 'Best_Baseline.keras')\n",
        "# model.save(model_path)  # Native Keras format\n",
        "# print(f\"Model saved as {model_path}\")\n"
      ],
      "metadata": {
        "id": "SCkjZ8kjxHNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quantisation of Baseline model"
      ],
      "metadata": {
        "id": "ByvZCsbCQxDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in Best Baseline model"
      ],
      "metadata": {
        "id": "GqzPUDKR-Wfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# --- Load baseline model ---\n",
        "model = tf.keras.models.load_model(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Baseline CNN Models/Best_Baseline.keras'\n",
        ")\n",
        "model_name_prefix = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Baseline CNN Models/Best_Baseline'"
      ],
      "metadata": {
        "id": "hiwxbSLk-You"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight-only quantisation"
      ],
      "metadata": {
        "id": "mZZHnZoF-vx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Float32 baseline (no quantization) ---\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "quantModel_f32 = converter.convert()\n",
        "\n",
        "# --- Weight-only quantization (int8 weights, float32 activations) ---\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # triggers int8 weight quantization\n",
        "quantModel_int8_weights = converter.convert()\n",
        "\n",
        "# --- Weight-only quantization (float16 weights, float32 activations) ---\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]  # store weights as float16\n",
        "quantModel_fp16_weights = converter.convert()\n",
        "\n",
        "# --- Save models ---\n",
        "with open(model_name_prefix + '_float32.tflite', 'wb') as f:\n",
        "    f.write(quantModel_f32)\n",
        "\n",
        "with open(model_name_prefix + '_int8_weights.tflite', 'wb') as f:\n",
        "    f.write(quantModel_int8_weights)\n",
        "\n",
        "with open(model_name_prefix + '_fp16_weights.tflite', 'wb') as f:\n",
        "    f.write(quantModel_fp16_weights)\n"
      ],
      "metadata": {
        "id": "MaS5aDcA-1Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of weight-only quant models (Dynamic Range Quantization)"
      ],
      "metadata": {
        "id": "nBHJXegv_fQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import psutil\n",
        "import time\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "50-DJCm4AuKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Resource usage measurement function ---\n",
        "def measure_resources(tflite_model, X_sample, model_name, save_path=\"models\"):\n",
        "\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    model_file = os.path.join(save_path, f\"{model_name.replace(' ', '_')}.tflite\")\n",
        "    with open(model_file, \"wb\") as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "    storage_size_mb = os.path.getsize(model_file) / (1024 * 1024) # Get size of model in megabytes\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem_before = process.memory_info().rss / (1024 * 1024)  # MB\n",
        "    cpu_before = psutil.cpu_percent(interval=None)\n",
        "\n",
        "    start_time = time.time()\n",
        "    input_data = np.expand_dims(X_sample, axis=0).astype(input_details[0]['dtype']) # Input data is only a single sample from the test set\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "    interpreter.invoke()\n",
        "    _ = interpreter.get_tensor(output_details[0]['index'])\n",
        "    end_time = time.time()\n",
        "\n",
        "    mem_after = process.memory_info().rss / (1024 * 1024)\n",
        "    cpu_after = psutil.cpu_percent(interval=None)\n",
        "\n",
        "    memory_used_mb = mem_after - mem_before\n",
        "    cpu_usage_percent = cpu_after - cpu_before\n",
        "    inference_time_sec = end_time - start_time\n",
        "\n",
        "    print(f\"\\n--- Resource Usage for {model_name} ---\")\n",
        "    print(f\"Storage size: {storage_size_mb:.2f} MB\")s\n",
        "    print(f\"Memory used during inference: {memory_used_mb:.2f} MB\")\n",
        "    print(f\"CPU usage change: {cpu_usage_percent:.2f}%\")\n",
        "    print(f\"Inference time: {inference_time_sec:.4f} sec\")\n",
        "\n",
        "    # return {\n",
        "    #     \"model_name\": model_name,\n",
        "    #     \"storage_mb\": storage_size_mb,\n",
        "    #     \"memory_mb\": memory_used_mb,\n",
        "    #     \"cpu_percent\": cpu_usage_percent,\n",
        "    #     \"inference_time_sec\": inference_time_sec\n",
        "    # }"
      ],
      "metadata": {
        "id": "j2caRTuLA57p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluate Quant Models ---\n",
        "\n",
        "# List of TFLite models\n",
        "tflite_models = [\n",
        "    {\"model\": quantModel_f32, \"name\": \"Float32 Model\"},\n",
        "    {\"model\": quantModel_int8_weights, \"name\": \"Int8 Weights-Only Model\"},\n",
        "    {\"model\": quantModel_fp16_weights, \"name\": \"Float16 Weights-Only Model\"}\n",
        "]\n",
        "\n",
        "# Evaluate the models\n",
        "for m in tflite_models:\n",
        "\n",
        "    tflite_model = m[\"model\"]\n",
        "    model_name = m[\"name\"]\n",
        "    print(f\"\\n --- Evaluating: {model_name} ---\")\n",
        "\n",
        "    # Loads the TFLite model and gets it ready to make predictions\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model) # Create interpreter object that will read and run the TFLite model\n",
        "    interpreter.allocate_tensors() # Make the interpreter allocate memory\n",
        "    input_details = interpreter.get_input_details() # Get expected shape and data type of the data the model needs to evaluate (built-in method)\n",
        "    output_details = interpreter.get_output_details() #Sshape and data type the model will need to produce the results in\n",
        "\n",
        "    # Get model predictions for test sample\n",
        "    y_pred_probs = []\n",
        "\n",
        "    for i in range(len(X_test)): # Interpreter object does not have a built-in .evaluate() method like the Keras Model object does, therefore need to go through every sample manually\n",
        "        input_data = np.expand_dims(X_test[i], axis=0).astype(input_details[0]['dtype']) # Add an extra dimension to the input (model expects input in batches, even if the batch size is 1) so the data type matches what the model expects\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_data) #Feed a single sample of input data into the TFLite interpreter\n",
        "        interpreter.invoke() # Tell the interpreter to run the model on the input data\n",
        "        output_data = interpreter.get_tensor(output_details[0]['index']) # Get the output from the model\n",
        "        y_pred_probs.append(output_data[0]) # Add the raw output to the list\n",
        "\n",
        "    y_pred_probs = np.array(y_pred_probs) # Convert the list to a single NumPy array\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1) # Get the predicted class label for each sample\n"
      ],
      "metadata": {
        "id": "P6w-6neu_ik4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Confusion Matrix ---\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    reverse_attack_type_map = {v: k for k, v in attack_type_map.items()}\n",
        "    labels = [reverse_attack_type_map.get(i, f'Unknown {i}') for i in range(cm.shape[0])]\n",
        "\n",
        "    plt.figure(figsize=(8, 4)) # Create confusion matrix plot\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "\n",
        "    # Print accuracy & classification report\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=labels))\n",
        "\n",
        "    # Calculate malware vs benign sample metrics\n",
        "    normal_label = attack_type_map.get('Normal Traffic', None)\n",
        "    if normal_label is not None:\n",
        "        malware_identified = np.sum(np.diag(cm)) - cm[normal_label, normal_label]\n",
        "        total_malware = np.sum(cm) - np.sum(cm[normal_label, :])\n",
        "        percentage_malware_identified = (malware_identified / total_malware * 100) if total_malware > 0 else 0\n",
        "\n",
        "        benign_misclassified = np.sum(cm[normal_label, :]) - cm[normal_label, normal_label]\n",
        "        total_benign = np.sum(cm[normal_label, :])\n",
        "        percentage_benign_misclassified = (benign_misclassified / total_benign * 100) if total_benign > 0 else 0\n",
        "\n",
        "        print(f\"Malware Identified: {malware_identified}/{total_malware} ({percentage_malware_identified:.2f}%)\")\n",
        "        print(f\"Benign Misclassified: {benign_misclassified}/{total_benign} ({percentage_benign_misclassified:.2f}%)\")\n",
        "        print(f\"TP: {malware_identified}, TN: {cm[normal_label, normal_label]}, FP: {benign_misclassified}, FN: {total_malware - malware_identified}\")\n",
        "\n",
        "    # execute resource usage measurement function\n",
        "    # _ = measure_resources(tflite_model, X_test[0], model_name)\n",
        "    measure_resources(tflite_model, X_test[0], model_name)\n"
      ],
      "metadata": {
        "id": "GoIUGr8SJtJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full integer Quantisation of the model"
      ],
      "metadata": {
        "id": "GMpJPw5lQ7jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "import numpy as np # Import numpy for representative dataset\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "u8qUMyd9LU8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Representitive Dataset ---\n",
        "\n",
        "def representative_dataset_gen():\n",
        "    # Use a small subset of your training data as the representative dataset\n",
        "    # The size of the subset can be adjusted (e.g., 100 samples)\n",
        "    num_samples = 100\n",
        "    for i in range(num_samples):\n",
        "        # Ensure the data type matches the model's input type (usually float32 for the original model)\n",
        "        # and the shape matches the model's input shape (excluding the batch dimension)\n",
        "        yield [X_train[i:i+1].astype(np.float32)] # Yield a list of numpy arrays"
      ],
      "metadata": {
        "id": "u6BtnYHSKyxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Full Integer Model Quantization ---\n",
        "\n",
        "# Create a float32 optimized model (no quantization, just optimization)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT] # Apply default optimizations\n",
        "converter.target_spec.supported_types = [tf.float32] # Specify target data type\n",
        "quantModel_f32 = converter.convert() # Convert the model\n",
        "\n",
        "\n",
        "# Create an int16 quantized model (requires representative dataset)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT] # Apply default optimizations\n",
        "converter.target_spec.supported_types = [tf.int16] # Specify target data type as int16\n",
        "converter.representative_dataset = representative_dataset_gen # Provide the representative dataset\n",
        "quantModel_int16 = converter.convert() # Convert the model\n",
        "\n",
        "\n",
        "# Create an int8 quantized model (requires representative dataset)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT] # Apply default optimizations\n",
        "converter.target_spec.supported_types = [tf.int8] # Specify target data type as int8\n",
        "converter.representative_dataset = representative_dataset_gen # Provide the representative dataset\n",
        "quantModel_int8 = converter.convert() # Convert the model\n",
        "\n",
        "# # Create an int4 quantized model -> NOT POSSIBLE\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT] # Apply default optimizations\n",
        "# converter.target_spec.supported_types = [tf.int4] # Specify target data type as int4\n",
        "# converter.inference_input_type = tf.int4 # Specify input type for inference\n",
        "# converter.inference_output_type = tf.int4 # Specify output type for inference\n",
        "# quantModel_int4 = converter.convert() # Convert the model\n",
        "\n",
        "\n",
        "# --- Save models ---\n",
        "os.makedirs('models', exist_ok=True)\n",
        "with open(model_name_prefix + '_float32_optimized.tflite', 'wb') as f: # Added _optimized to filename\n",
        "    f.write(quantModel_f32)\n",
        "with open(model_name_prefix + '_int16_full.tflite', 'wb') as f: # Added _full to filename\n",
        "    f.write(quantModel_int16)\n",
        "with open(model_name_prefix + '_int8_full.tflite', 'wb') as f: # Added _full to filename\n",
        "    f.write(quantModel_int8)"
      ],
      "metadata": {
        "id": "qTSEgAHbQ97u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1faa2df6-a1e2-459b-faab-1bcf698e7dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpayxerxdm'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 52, 1), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  133997749130000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726000784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726000400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726000208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726000592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726001360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726000976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997725995408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726001744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997725999056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997725999248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726002128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726001168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726001552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997725998288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997725997712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Saved artifact at '/tmp/tmpw577lagg'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 52, 1), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  133997749130000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726000784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726000400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726000208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726000592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726001360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726000976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997725995408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726001744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997725999056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997725999248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726002128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726001168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997726001552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997725998288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133997725997712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "For full integer quantization, a `representative_dataset` must be specified.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-950128345.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# converter.inference_input_type = tf.int8 # Remove this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# converter.inference_output_type = tf.int8 # Remove this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mquantModel_int8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# # Create an int4 quantized model -> NOT POSSIBLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1248\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \"\"\"\n\u001b[1;32m   1199\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_increase_conversion_attempt_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_save_conversion_params_metric\u001b[0;34m(self, graph_def, inference_type, inference_input_type)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;31m# Optimization parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m     quant_mode = QuantizationMode(\n\u001b[0m\u001b[1;32m   1023\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, optimizations, target_spec, representative_dataset, graph_def, disable_per_channel, experimental_new_dynamic_range_quantizer, experimental_low_bit_qat, full_integer_quantization_bias_type, experimental_mlir_variable_quantization, experimental_qdq_annotation)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_int8_target_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_int8_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     self.enable_mlir_variable_quantization = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_validate_int8_required\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_quantization_aware_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     ):\n\u001b[0;32m--> 510\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;34m\"For full integer quantization, a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0;34m\"`representative_dataset` must be specified.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: For full integer quantization, a `representative_dataset` must be specified."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of quant models"
      ],
      "metadata": {
        "id": "CLTfkBfzRPCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Imports ---\n",
        "import os\n",
        "import psutil\n",
        "import time\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "0a_N00V-LvT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- List of TFLite models ---\n",
        "tflite_models = [\n",
        "    {\"model\": quantModel_f32, \"name\": \"Float32 Optimized Model\"},\n",
        "    {\"model\": quantModel_int16, \"name\": \"Int16 Quantized Model\"},\n",
        "    {\"model\": quantModel_int8, \"name\": \"Int8 Quantized Model\"}\n",
        "]\n",
        "\n",
        "# --- Evaluate all models with metrics + resource tracking ---\n",
        "for m in tflite_models:\n",
        "    tflite_model = m[\"model\"]\n",
        "    model_name = m[\"name\"]\n",
        "    print(f\"\\n=== Evaluating: {model_name} ===\")\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_probs = []\n",
        "    for i in range(len(X_test)):\n",
        "        input_data = np.expand_dims(X_test[i], axis=0).astype(input_details[0]['dtype'])\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "        interpreter.invoke()\n",
        "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "        y_pred_probs.append(output_data[0])\n",
        "\n",
        "    y_pred_probs = np.array(y_pred_probs)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)"
      ],
      "metadata": {
        "id": "WuEdsrjtRRK-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "e96598c7-6461-4944-b6cf-2dc9a7d52f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'quantModel_int8' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1986699228.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquantModel_f32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Float32 Optimized Model\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquantModel_int16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Int16 Quantized Model\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquantModel_int8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Int8 Quantized Model\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m ]\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'quantModel_int8' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Confusion Matrix ---\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    reverse_attack_type_map = {v: k for k, v in attack_type_map.items()}\n",
        "    labels = [reverse_attack_type_map.get(i, f'Unknown {i}') for i in range(cm.shape[0])]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "\n",
        "    # Accuracy & classification report\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=labels))\n",
        "\n",
        "    # Malware vs Benign metrics\n",
        "    normal_label = attack_type_map.get('Normal Traffic', None)\n",
        "    if normal_label is not None:\n",
        "        malware_identified = np.sum(np.diag(cm)) - cm[normal_label, normal_label]\n",
        "        total_malware = np.sum(cm) - np.sum(cm[normal_label, :])\n",
        "        percentage_malware_identified = (malware_identified / total_malware * 100) if total_malware > 0 else 0\n",
        "\n",
        "        benign_misclassified = np.sum(cm[normal_label, :]) - cm[normal_label, normal_label]\n",
        "        total_benign = np.sum(cm[normal_label, :])\n",
        "        percentage_benign_misclassified = (benign_misclassified / total_benign * 100) if total_benign > 0 else 0\n",
        "\n",
        "        print(f\"Malware Identified: {malware_identified}/{total_malware} ({percentage_malware_identified:.2f}%)\")\n",
        "        print(f\"Benign Misclassified: {benign_misclassified}/{total_benign} ({percentage_benign_misclassified:.2f}%)\")\n",
        "        print(f\"TP: {malware_identified}, TN: {cm[normal_label, normal_label]}, FP: {benign_misclassified}, FN: {total_malware - malware_identified}\")\n",
        "\n",
        "    # Track resources\n",
        "    measure_tflite_resources(tflite_model, X_test[0], model_name)"
      ],
      "metadata": {
        "id": "EtyrNcirMBg3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}