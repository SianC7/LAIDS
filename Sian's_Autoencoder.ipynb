{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1jJ4QDrSAEifUiGNpyiUDtNn0D4dmqt1I",
      "authorship_tag": "ABX9TyPStLkei2JJuD8AqVXiP5n0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SianC7/LAIDS/blob/main/Sian's_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "MKlmLT7z2-o8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collection:"
      ],
      "metadata": {
        "id": "wuiu17so3BdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03NYQCHm2684",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e59d73-2663-4554-ef2c-c5cc23bca86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Monday-WorkingHours.pcap_ISCX.csv with 529918 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Tuesday-WorkingHours.pcap_ISCX.csv with 445909 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Wednesday-workingHours.pcap_ISCX.csv with 692703 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv with 170366 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv with 288602 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Friday-WorkingHours-Morning.pcap_ISCX.csv with 191033 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv with 225745 rows\n",
            "Loaded file: /content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv with 286467 rows\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set pandas display options for wide output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "# List of file paths\n",
        "data_file_paths = [\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Monday-WorkingHours.pcap_ISCX.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Tuesday-WorkingHours.pcap_ISCX.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Wednesday-workingHours.pcap_ISCX.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 Machine Learning CVE csv files/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
        "\n",
        "\n",
        "]\n",
        "#Set database variables\n",
        "Mon_df = None\n",
        "Tue_df = None\n",
        "Wed_df = None\n",
        "Thur_df_list = []\n",
        "Fri_df_list = []\n",
        "\n",
        "for file_path in data_file_paths: # Loop through the file paths and add the data to a temp df var\n",
        "  try:\n",
        "      temp_df = pd.read_csv(file_path, sep=\",\", comment=\"#\", header=0)\n",
        "      temp_df.columns = temp_df.columns.str.strip()  # Strip whitespace from column names\n",
        "\n",
        "      if \"Monday\" in file_path:\n",
        "        Mon_df = temp_df\n",
        "        print(f\"Loaded file: {file_path} with {len(temp_df)} rows\")\n",
        "      elif \"Tuesday\" in file_path:\n",
        "        Tue_df = temp_df\n",
        "        print(f\"Loaded file: {file_path} with {len(temp_df)} rows\")\n",
        "      elif \"Wednesday\" in file_path:\n",
        "        Wed_df = temp_df\n",
        "        print(f\"Loaded file: {file_path} with {len(temp_df)} rows\")\n",
        "      elif \"Thursday\" in file_path:\n",
        "        Thur_df_list.append(temp_df)\n",
        "        print(f\"Loaded file: {file_path} with {len(temp_df)} rows\")\n",
        "      elif \"Friday\" in file_path:\n",
        "        Fri_df_list.append(temp_df)\n",
        "        print(f\"Loaded file: {file_path} with {len(temp_df)} rows\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error reading file {file_path}: {e}\")\n",
        "\n",
        "\n",
        "#Concat df lists:\n",
        "Thur_df = pd.concat(Thur_df_list, ignore_index=True) #TEST\n",
        "Fri_df = pd.concat(Fri_df_list, ignore_index=True)\n",
        "\n",
        "# #TEST\n",
        "# print(type(Mon_df))   # should be <class 'pandas.core.frame.DataFrame'>\n",
        "# print(type(Tue_df))   # same\n",
        "# print(type(Wed_df))   # same\n",
        "# print(type(Thur_df))   # same\n",
        "# print(type(Fri_df))   # same\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "day_dfs = [(\"Monday\", Mon_df), (\"Tuesday\", Tue_df), (\"Wednesday\", Wed_df), (\"Thursday\", Thur_df), (\"Friday\", Fri_df)]\n",
        "\n",
        "# print(\"\\nInitial samples:\")\n",
        "# for day, df in day_dfs:\n",
        "#     print(f\"\\n{day}:\")\n",
        "#     print(f\"Number of columns: {df.shape[1]}\")\n",
        "#     print(df.head().to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Distribution:"
      ],
      "metadata": {
        "id": "pOpCHXVnCuF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"\\nLabel distribution in each dataframe:\")\n",
        "\n",
        "# total_benign = 0\n",
        "# total_malicious = 0\n",
        "\n",
        "# for day, df in day_dfs:\n",
        "#     print(f\"\\n--- {day} ---\")\n",
        "#     if df.empty:\n",
        "#         print(\"No data for this day.\")\n",
        "#         continue\n",
        "\n",
        "#     if 'Label' in df.columns:\n",
        "#         label_counts = df['Label'].value_counts(dropna=False)\n",
        "#         print(label_counts.to_string())\n",
        "\n",
        "#         for label, count in label_counts.items():\n",
        "#             if str(label).strip().upper() == 'BENIGN':\n",
        "#                 total_benign += count\n",
        "#             else:\n",
        "#                 total_malicious += count\n",
        "#     else:\n",
        "#         print(\"⚠️ 'Label' column not found.\")\n",
        "\n",
        "# print(\"\\n--- Total Sample Summary ---\")\n",
        "# print(f\"Total BENIGN samples: {total_benign}\")\n",
        "# print(f\"Total MALICIOUS samples: {total_malicious}\")"
      ],
      "metadata": {
        "id": "CkCkRPOACvyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning:"
      ],
      "metadata": {
        "id": "ZQCFkYOr3C9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set pandas display options for wide output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "for df_name, df in day_dfs:\n",
        "    # Replace any '-' with NaN for consistent handling\n",
        "    df.replace('-', np.nan, inplace=True)\n",
        "\n",
        "    # Replace inf/-inf with NaN (handles both float and int infinities)\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Round all float columns to 4 decimal places\n",
        "    df = df.round(4)\n",
        "\n",
        "    # Drop columns with a single unique value (constant columns), e.g. all zeros\n",
        "    #df.drop(columns=[col for col in df.columns if df[col].nunique(dropna=False) == 1 or df[col].eq(0).all()], inplace=True) #FIXX\n",
        "\n",
        "    # --- Drop 'Flow Bytes/s' and 'Flow Packets/s' columns ---\n",
        "    df.drop(columns=['Flow Bytes/s', 'Flow Packets/s'], errors='ignore', inplace=True)\n",
        "\n",
        "    # Convert labels into binary: benign=0, malicious=1\n",
        "    df['Label'] = df['Label'].apply(lambda x: 0 if 'BENIGN' in str(x).upper() else 1)\n",
        "\n",
        "    # Drop any rows containing NaN\n",
        "    df.dropna(inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZ5jTKwr3FMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/Test Split"
      ],
      "metadata": {
        "id": "yNULnbGY5kbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # --- Train/Test Split ---\n",
        "\n",
        "# # Training data (Monday - benign only)\n",
        "# Mon_df.dropna(inplace=True)\n",
        "# X_train = Mon_df.drop(columns=['Label']).values\n",
        "# y_train = Mon_df['Label'].values\n",
        "\n",
        "# # Test set contains Malicious and Benign data\n",
        "# Tue_to_Fri_df = pd.concat([Tue_df, Wed_df, Thur_df, Fri_df], ignore_index=True)\n",
        "# Tue_to_Fri_df.dropna(inplace=True)\n",
        "# #Extract some benign samples for the training set\n",
        "# benign_df = Tue_to_Fri_df[Tue_to_Fri_df['Label'] == 0]\n",
        "# malicious_df = Tue_to_Fri_df[Tue_to_Fri_df['Label'] == 1]\n",
        "\n",
        "# # Split 20% of benign into training, rest stay in test\n",
        "# benign_train, benign_test = train_test_split(benign_df, test_size=0.80, random_state=42, shuffle=True)\n",
        "\n",
        "# # Step 3: Create the final training set\n",
        "# X_train = pd.concat([Mon_df, benign_train], ignore_index=True).drop(columns=['Label']).values\n",
        "# y_train = pd.concat([Mon_df, benign_train], ignore_index=True)['Label'].values\n",
        "\n",
        "# # Step 4: Create the final test set (remaining benign + all malicious)\n",
        "# test_df = pd.concat([benign_test, malicious_df], ignore_index=True)\n",
        "# X_test = test_df.drop(columns=['Label']).values\n",
        "# y_test = test_df['Label'].values\n",
        "\n",
        "# y_test = test_df['Label'].values\n",
        "# X_test = test_df.drop(columns=['Label']).values\n",
        "\n",
        "# # Print dataset sizes\n",
        "# print(f\"Training set size: {len(X_train)} samples\")\n",
        "# print(f\"Test set size: {len(X_test)} samples\")\n",
        "\n",
        "\n",
        "# # Print dataset sizes\n",
        "# print(f\"Training set size: {len(X_train)} samples\")\n",
        "# print(f\"Training set shape: {X_train.shape}\")\n",
        "\n",
        "# print(f\"Test set size: {len(X_test)} samples\")\n",
        "# print(f\"Test set shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "hMQ-3vY55qOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# import pandas as pd\n",
        "\n",
        "# # --- Clean all relevant datasets ---\n",
        "# Mon_df.dropna(inplace=True)\n",
        "# Tue_df.dropna(inplace=True)\n",
        "# Wed_df.dropna(inplace=True)\n",
        "# Thur_df.dropna(inplace=True)\n",
        "# Fri_df.dropna(inplace=True)\n",
        "\n",
        "# # --- Combine Tue–Fri for test/benign sampling ---\n",
        "# Tue_to_Fri_df = pd.concat([Tue_df, Wed_df, Thur_df, Fri_df], ignore_index=True)\n",
        "\n",
        "# # Separate benign and malicious traffic\n",
        "# benign_df = Tue_to_Fri_df[Tue_to_Fri_df['Label'] == 0]  # 0 = Benign\n",
        "# malicious_df = Tue_to_Fri_df[Tue_to_Fri_df['Label'] == 1]  # 1 = Malicious\n",
        "\n",
        "# # --- Split 20% benign for training, 80% for testing ---\n",
        "# benign_train, benign_test = train_test_split(\n",
        "#     benign_df, test_size=0.80, random_state=42, shuffle=True\n",
        "# )\n",
        "\n",
        "# # --- Final Training Set (Benign only) ---\n",
        "# train_df = pd.concat([Mon_df, benign_train], ignore_index=True)\n",
        "# X_train_full = train_df.drop(columns=['Label']).values  # full benign training\n",
        "# y_train_full = train_df['Label'].values  # should all be 0\n",
        "\n",
        "# # --- Optional: Split train/val from full train (e.g., 80/20) ---\n",
        "# X_train, X_val, _, _ = train_test_split(\n",
        "#     X_train_full, y_train_full, test_size=0.2, random_state=42, shuffle=True\n",
        "# )\n",
        "\n",
        "# # --- Final Test Set (Malicious + remaining benign) ---\n",
        "# test_df = pd.concat([benign_test, malicious_df], ignore_index=True)\n",
        "# X_test = test_df.drop(columns=['Label']).values\n",
        "# y_test = test_df['Label'].values\n",
        "\n",
        "# # --- Print dataset info ---\n",
        "# print(f\"X_train shape: {X_train.shape}\")\n",
        "# print(f\"X_val shape:   {X_val.shape}\")\n",
        "# print(f\"X_test shape:  {X_test.shape}\")\n",
        "# print(f\"Test benign count: {(y_test == 0).sum()}\")\n",
        "# print(f\"Test malicious count: {(y_test == 1).sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "C4xPcsCEHj2z",
        "outputId": "cb3151d7-2103-4298-bd9d-0186b85dd06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.8 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-1830696320.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# --- Split 20% benign for training, 80% for testing ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m benign_train, benign_test = train_test_split(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mbenign_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.8 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b802ed4a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Any NaNs in X_train?\", np.isnan(X_train).any())\n",
        "\n",
        "# Check for infinite values in X_train and X_test\n",
        "print(\"Checking for infinite values before scaling:\")\n",
        "\n",
        "if np.isinf(X_train).any():\n",
        "    print(\"Infinite values found in X_train.\")\n",
        "else:\n",
        "    print(\"No infinite values found in X_train.\")\n",
        "\n",
        "if np.isinf(X_test).any():\n",
        "    print(\"Infinite values found in X_test.\")\n",
        "else:\n",
        "    print(\"No infinite values found in X_test.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalise the data:"
      ],
      "metadata": {
        "id": "3Qg4Tb925afz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler # Keep both imports in case user wants to switch\n",
        "\n",
        "# --- Normalize --- #standardise data features\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply the scaler\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Print the shape of the scaled data to verify\n",
        "print(f\"Shape of X_train_scaled after scaling: {X_train_scaled.shape}\")\n",
        "print(f\"Shape of X_test_scaled after scaling: {X_test_scaled.shape}\")\n"
      ],
      "metadata": {
        "id": "AhVdUYUI5tPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autoencoder model\n",
        "An autoencoder model consists of two parts:\n",
        "- **Encoder**: Responsible for compressing the input data into a lower-dimensional representation of the input data that captures the most important information.\n",
        "- **Decoder**: Responsible for reconstructing the input data from the latent space representation. The decoder takes the latent space representation as input and produces a reconstructed version of the input data as output. The reconstructed data should be as similar as possible to the original input data.\n",
        "\n",
        "- Advantages of autoencoders: Can be used to detect anomalies, unsupervised learning models (they do not require labeled data in training), easy to train and implement.\n",
        "- Disadvantages of autoencoders: computationally expensive to train (especially for large datasets.), they are sensitive to the choice of hyperparameters, difficult to interpret.\n",
        "\n",
        "Below is a simple autoencoder model with three layers in the encoder and three layers in the decoder. https://levelup.gitconnected.com/build-deep-autoencoders-model-for-anomaly-detection-in-python-a-complete-guide-a7d0ec0e688\n",
        "https://medium.com/@ndhilani.simbine/how-i-built-a-real-time-anomaly-detection-system-for-enterprise-networks-using-python-and-machine-0078d8a26e84\n",
        "https://medium.com/@walid.daboubi/malware-detection-using-deep-autoencoder-neural-network-wannacry-as-a-test-9d0125c925b2"
      ],
      "metadata": {
        "id": "kB51A9_Q_g_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Determine the input dimension from the scaled training data\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "print(f\"Autoencoder input dimension: {input_dim}\")\n",
        "\n",
        "autoencoder = Sequential([\n",
        "    Input(shape=(input_dim,)),\n",
        "    Dense(64, activation='relu'),                             # Encoder layer\n",
        "    Dense(32, activation='relu'),                             # Encoder layer\n",
        "    Dense(16, activation='relu'),                             # Encoder bottleneck\n",
        "    Dense(32, activation='relu'),                             # Decoder layer\n",
        "    Dense(64, activation='relu'),                             # Decoder layer\n",
        "    Dense(input_dim, activation='sigmoid')                    # Output layer (same size as input)\n",
        "])\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "rN4vtElJ_jaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and test the model"
      ],
      "metadata": {
        "id": "ANpOFlaNGodh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "\n",
        "# Start timing\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "# Train the autoencoder\n",
        "history = autoencoder.fit(\n",
        "    X_train_scaled, X_train_scaled,  # Autoencoders use X_train as both input and target #\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# End timing\n",
        "end_time = timeit.default_timer()\n",
        "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Autoencoder Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "be8XMHLWGrDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test the model"
      ],
      "metadata": {
        "id": "ICz3kCpnfhVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "loss = autoencoder.evaluate(X_test_scaled, X_test_scaled)\n",
        "mse = autoencoder.evaluate(X_test_scaled, X_test_scaled)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test mean squared error:', mse)\n",
        "\n",
        "# Calculate the anomaly score (by calculating the reconstruction errors for the test data)\n",
        "# Calculate the reconstruction errors for the test data\n",
        "# reconstruction_errors = autoencoder.predict(X_test_scaled) - X_test_scaled\n",
        "reconstructions = autoencoder.predict(X_test_scaled)\n",
        "reconstruction_errors = np.mean(np.square(reconstructions - X_test_scaled), axis=1)\n",
        "\n",
        "\n",
        "# Threshold the reconstruction errors to identify anomalies\n",
        "#anomaly_threshold = 0.1\n",
        "#anomalies = np.where(reconstruction_errors > anomaly_threshold)[0]\n",
        "threshold = np.percentile(reconstruction_errors, 95)\n",
        "anomalies = np.where(reconstruction_errors > threshold)[0]\n",
        "\n",
        "# Print the number of anomalies detected\n",
        "print('Number of anomalies detected:', len(anomalies))"
      ],
      "metadata": {
        "id": "tmcSjua3fkdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reconstruction Error Plot?"
      ],
      "metadata": {
        "id": "4BKWp0Uwfacb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# # --- Train the model ---\n",
        "# autoencoder.fit(X_train, X_train, epochs=20, validation_split=0.2)\n",
        "\n",
        "# # --- Evaluate the model ---\n",
        "# loss = autoencoder.evaluate(X_test, X_test)\n",
        "# print('Test loss (MSE):', loss)\n",
        "\n",
        "# # --- Predict reconstructed outputs ---\n",
        "# reconstructed = autoencoder.predict(X_test)\n",
        "\n",
        "# # --- Calculate per-sample reconstruction error (mean squared error) ---\n",
        "# reconstruction_errors = np.mean((X_test - reconstructed) ** 2, axis=1)\n",
        "\n",
        "# --- Visualize reconstruction error ---\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(reconstruction_errors, bins=50, color='skyblue', edgecolor='black')\n",
        "plt.axvline(np.mean(reconstruction_errors) + 2 * np.std(reconstruction_errors), color='red', linestyle='--', label='Anomaly Threshold')\n",
        "plt.title(\"Reconstruction Error Distribution\")\n",
        "plt.xlabel(\"Mean Squared Error per Sample\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Detect anomalies based on threshold ---\n",
        "threshold = np.mean(reconstruction_errors) + 2 * np.std(reconstruction_errors)\n",
        "anomalies = np.where(reconstruction_errors > threshold)[0]\n",
        "\n",
        "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
      ],
      "metadata": {
        "id": "udRl9xe6fc88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Pick a sample index to visualize\n",
        "sample_idx = 0\n",
        "\n",
        "# Get input and reconstruction\n",
        "input_sample = X_test[sample_idx]\n",
        "reconstructed_sample = autoencoder.predict(X_test[sample_idx][np.newaxis, ...])[0]\n",
        "\n",
        "# Plot input and reconstruction\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(input_sample, label='Input', color='blue')\n",
        "plt.plot(reconstructed_sample, label='Reconstruction', color='red')\n",
        "\n",
        "# Plot error band\n",
        "plt.fill_between(np.arange(len(input_sample)),\n",
        "                 np.minimum(input_sample, reconstructed_sample),\n",
        "                 np.maximum(input_sample, reconstructed_sample),\n",
        "                 color='red', alpha=0.3, label='Error')\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Autoencoder Input vs Reconstruction\")\n",
        "plt.xlabel(\"Feature Index\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Gs6735XUe_Ci"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}