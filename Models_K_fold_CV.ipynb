{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SianC7/LAIDS/blob/main/Models_K_fold_CV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da52_MEEVADn"
      },
      "source": [
        "#Load in CICIDS2017 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ipHK7lGDVDtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb6fb57-9b45-48a1-f9bd-90b1b4fea035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial samples:\n",
            "cicids2017_df shape: (4397849, 53)\n",
            "\n",
            "Attack Type Distribution:\n",
            "Attack Type\n",
            "Normal Traffic    2000000\n",
            "DoS                400853\n",
            "Brute Force        400059\n",
            "Web Attacks        400007\n",
            "Bots               399828\n",
            "DDoS               399757\n",
            "Port Scanning      397345\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label Encoding Mapping:\n",
            "{'Normal Traffic': 0, 'Port Scanning': 1, 'Web Attacks': 2, 'Brute Force': 3, 'DDoS': 4, 'Bots': 5, 'DoS': 6}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Data Collection ---\n",
        "\n",
        "# Set pandas display options for wide output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "# Get Data file path\n",
        "# file_path = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/cicids2017_cleaned.csv'\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Datasets/CICIDS2017 ADASYN Dataset/ADASYN_CICIDS2017_Dataset6.csv'\n",
        "\n",
        "#file_path = '/Users/siancaine/Downloads/cicids2017_cleaned.csv'\n",
        "# file_path = '/Users/siancaine/Downloads/ADASYN_CICIDS2017_Dataset6.csv'\n",
        "\n",
        "cicids2017_df = pd.read_csv(file_path, sep=\",\", comment=\"#\", header=0)\n",
        "cicids2017_df.columns = cicids2017_df.columns.str.strip()  # Strip whitespace from column names\n",
        "\n",
        "print(\"\\nInitial samples:\")\n",
        "print(f\"cicids2017_df shape: {cicids2017_df.shape}\")\n",
        "# print(cicids2017_df.head().to_string())\n",
        "# print(cicids2017_df.info())\n",
        "\n",
        "# Print unique values and their counts for 'Attack Type'\n",
        "print(\"\\nAttack Type Distribution:\")\n",
        "print(cicids2017_df['Attack Type'].value_counts())\n",
        "\n",
        "# --- Label Encoding ---\n",
        "\n",
        "# Get unique attack types\n",
        "attack_types = cicids2017_df['Attack Type'].unique()\n",
        "\n",
        "# Create a mapping from attack type to integer label\n",
        "attack_type_map = {'Normal Traffic': 0, 'Port Scanning': 1, 'Web Attacks': 2, 'Brute Force': 3, 'DDoS': 4, 'Bots': 5, 'DoS': 6} # Use the specified mapping\n",
        "\n",
        "# Apply label encoding\n",
        "cicids2017_df['Attack Type'] = cicids2017_df['Attack Type'].map(attack_type_map)\n",
        "\n",
        "print(\"\\nLabel Encoding Mapping:\")\n",
        "print(attack_type_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzW6DRXemJWz"
      },
      "source": [
        "# Instantiate StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "73NZhxxxmQcq"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ats0Ocgzi5hH"
      },
      "source": [
        "#BASELINE K-FOLD CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TnztT2bUtbO",
        "outputId": "35c2822b-81b2-4944-96b0-7144534f3e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1:\n",
            "Train_index: [      0       1       3 ... 4397845 4397847 4397848]\n",
            "Test_index: [      2      11      14 ... 4397841 4397842 4397846]\n",
            "Training fold input shape: (3518279, 52, 1)\n",
            "Testing fold input shape: (879570, 52, 1)\n",
            "Single sample's input shape: (52, 1)\n",
            "Number of classes: 7\n",
            "Epoch 1/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 3ms/step - accuracy: 0.8980 - loss: 0.2555 - val_accuracy: 0.9720 - val_loss: 0.1020\n",
            "Epoch 2/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 3ms/step - accuracy: 0.9534 - loss: 0.1399 - val_accuracy: 0.7881 - val_loss: 0.6012\n",
            "Epoch 3/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 3ms/step - accuracy: 0.9628 - loss: 0.1189 - val_accuracy: 0.9785 - val_loss: 0.0701\n",
            "Epoch 4/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 3ms/step - accuracy: 0.9677 - loss: 0.1074 - val_accuracy: 0.9507 - val_loss: 0.1252\n",
            "Epoch 5/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 3ms/step - accuracy: 0.9697 - loss: 0.1008 - val_accuracy: 0.5828 - val_loss: 1.0197\n",
            "\u001b[1m27487/27487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step\n",
            "Fold 1 Accuracy: 0.9787\n",
            "Fold 1 Confusion Matric report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98    400000\n",
            "           1       0.93      0.99      0.96     79469\n",
            "           2       0.97      1.00      0.98     80001\n",
            "           3       1.00      1.00      1.00     80012\n",
            "           4       0.99      1.00      0.99     79951\n",
            "           5       0.95      1.00      0.97     79966\n",
            "           6       0.95      0.99      0.97     80171\n",
            "\n",
            "    accuracy                           0.98    879570\n",
            "   macro avg       0.97      0.99      0.98    879570\n",
            "weighted avg       0.98      0.98      0.98    879570\n",
            "\n",
            "Fold 2:\n",
            "Train_index: [      0       1       2 ... 4397846 4397847 4397848]\n",
            "Test_index: [      3       6       7 ... 4397815 4397825 4397836]\n",
            "Training fold input shape: (3518279, 52, 1)\n",
            "Testing fold input shape: (879570, 52, 1)\n",
            "Single sample's input shape: (52, 1)\n",
            "Number of classes: 7\n",
            "Epoch 1/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3ms/step - accuracy: 0.8974 - loss: 0.2585 - val_accuracy: 0.9551 - val_loss: 0.1262\n",
            "Epoch 2/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 3ms/step - accuracy: 0.9429 - loss: 0.1618 - val_accuracy: 0.9767 - val_loss: 0.0763\n",
            "Epoch 3/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 3ms/step - accuracy: 0.9501 - loss: 0.1465 - val_accuracy: 0.9768 - val_loss: 0.0804\n",
            "Epoch 4/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 3ms/step - accuracy: 0.9582 - loss: 0.1308 - val_accuracy: 0.9787 - val_loss: 0.0713\n",
            "Epoch 5/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 3ms/step - accuracy: 0.9611 - loss: 0.1242 - val_accuracy: 0.9799 - val_loss: 0.0692\n",
            "\u001b[1m27487/27487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step\n",
            "Fold 2 Accuracy: 0.9794\n",
            "Fold 2 Confusion Matric report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98    400000\n",
            "           1       0.93      0.99      0.96     79469\n",
            "           2       0.97      1.00      0.98     80001\n",
            "           3       0.99      1.00      0.99     80012\n",
            "           4       0.98      1.00      0.99     79951\n",
            "           5       0.96      1.00      0.98     79966\n",
            "           6       0.96      0.99      0.97     80171\n",
            "\n",
            "    accuracy                           0.98    879570\n",
            "   macro avg       0.97      0.99      0.98    879570\n",
            "weighted avg       0.98      0.98      0.98    879570\n",
            "\n",
            "Fold 3:\n",
            "Train_index: [      0       1       2 ... 4397846 4397847 4397848]\n",
            "Test_index: [     12      24      26 ... 4397835 4397839 4397844]\n",
            "Training fold input shape: (3518279, 52, 1)\n",
            "Testing fold input shape: (879570, 52, 1)\n",
            "Single sample's input shape: (52, 1)\n",
            "Number of classes: 7\n",
            "Epoch 1/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 3ms/step - accuracy: 0.9029 - loss: 0.2514 - val_accuracy: 0.9718 - val_loss: 0.0930\n",
            "Epoch 2/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 3ms/step - accuracy: 0.9587 - loss: 0.1309 - val_accuracy: 0.9692 - val_loss: 0.0968\n",
            "Epoch 3/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 3ms/step - accuracy: 0.9648 - loss: 0.1151 - val_accuracy: 0.9786 - val_loss: 0.0688\n",
            "Epoch 4/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 3ms/step - accuracy: 0.9693 - loss: 0.1031 - val_accuracy: 0.9793 - val_loss: 0.0686\n",
            "Epoch 5/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 3ms/step - accuracy: 0.9707 - loss: 0.0995 - val_accuracy: 0.9800 - val_loss: 0.0662\n",
            "\u001b[1m27487/27487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step\n",
            "Fold 3 Accuracy: 0.9802\n",
            "Fold 3 Confusion Matric report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98    400000\n",
            "           1       0.93      0.99      0.96     79469\n",
            "           2       0.97      1.00      0.99     80002\n",
            "           3       1.00      1.00      1.00     80011\n",
            "           4       0.98      1.00      0.99     79952\n",
            "           5       0.96      1.00      0.98     79966\n",
            "           6       0.96      0.99      0.97     80170\n",
            "\n",
            "    accuracy                           0.98    879570\n",
            "   macro avg       0.97      0.99      0.98    879570\n",
            "weighted avg       0.98      0.98      0.98    879570\n",
            "\n",
            "Fold 4:\n",
            "Train_index: [      0       2       3 ... 4397844 4397845 4397846]\n",
            "Test_index: [      1       4       5 ... 4397840 4397847 4397848]\n",
            "Training fold input shape: (3518279, 52, 1)\n",
            "Testing fold input shape: (879570, 52, 1)\n",
            "Single sample's input shape: (52, 1)\n",
            "Number of classes: 7\n",
            "Epoch 1/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 3ms/step - accuracy: 0.9027 - loss: 0.2519 - val_accuracy: 0.9740 - val_loss: 0.0807\n",
            "Epoch 2/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 3ms/step - accuracy: 0.9607 - loss: 0.1268 - val_accuracy: 0.9765 - val_loss: 0.0730\n",
            "Epoch 3/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 3ms/step - accuracy: 0.9666 - loss: 0.1116 - val_accuracy: 0.9786 - val_loss: 0.0706\n",
            "Epoch 4/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 3ms/step - accuracy: 0.9691 - loss: 0.1042 - val_accuracy: 0.9776 - val_loss: 0.0704\n",
            "Epoch 5/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 3ms/step - accuracy: 0.9702 - loss: 0.1010 - val_accuracy: 0.9809 - val_loss: 0.0710\n",
            "\u001b[1m27487/27487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step\n",
            "Fold 4 Accuracy: 0.9775\n",
            "Fold 4 Confusion Matric report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98    400000\n",
            "           1       0.93      0.99      0.96     79469\n",
            "           2       0.97      0.99      0.98     80002\n",
            "           3       0.99      1.00      1.00     80012\n",
            "           4       0.98      1.00      0.99     79952\n",
            "           5       0.94      1.00      0.97     79965\n",
            "           6       0.96      0.99      0.97     80170\n",
            "\n",
            "    accuracy                           0.98    879570\n",
            "   macro avg       0.97      0.99      0.98    879570\n",
            "weighted avg       0.98      0.98      0.98    879570\n",
            "\n",
            "Fold 5:\n",
            "Train_index: [      1       2       3 ... 4397846 4397847 4397848]\n",
            "Test_index: [      0       9      21 ... 4397834 4397843 4397845]\n",
            "Training fold input shape: (3518280, 52, 1)\n",
            "Testing fold input shape: (879569, 52, 1)\n",
            "Single sample's input shape: (52, 1)\n",
            "Number of classes: 7\n",
            "Epoch 1/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 3ms/step - accuracy: 0.8974 - loss: 0.2575 - val_accuracy: 0.9709 - val_loss: 0.1004\n",
            "Epoch 2/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 3ms/step - accuracy: 0.9510 - loss: 0.1476 - val_accuracy: 0.9727 - val_loss: 0.0917\n",
            "Epoch 3/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3ms/step - accuracy: 0.9596 - loss: 0.1275 - val_accuracy: 0.9793 - val_loss: 0.0680\n",
            "Epoch 4/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 3ms/step - accuracy: 0.9665 - loss: 0.1111 - val_accuracy: 0.9801 - val_loss: 0.0661\n",
            "Epoch 5/5\n",
            "\u001b[1m49476/49476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 3ms/step - accuracy: 0.9679 - loss: 0.1088 - val_accuracy: 0.9788 - val_loss: 0.0694\n",
            "\u001b[1m27487/27487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step\n",
            "Fold 5 Accuracy: 0.9798\n",
            "Fold 5 Confusion Matric report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98    400000\n",
            "           1       0.93      0.99      0.96     79469\n",
            "           2       0.97      1.00      0.98     80001\n",
            "           3       1.00      1.00      1.00     80012\n",
            "           4       0.98      1.00      0.99     79951\n",
            "           5       0.96      1.00      0.98     79965\n",
            "           6       0.97      0.98      0.97     80171\n",
            "\n",
            "    accuracy                           0.98    879569\n",
            "   macro avg       0.97      0.99      0.98    879569\n",
            "weighted avg       0.98      0.98      0.98    879569\n",
            "\n",
            "\n",
            "List of accuracy scores: [0.978732789885967, 0.9794422274520505, 0.9801653080482509, 0.9775242448014371, 0.9797628156517567]\n",
            "Maximum Accuracy: 98.02%\n",
            "Minimum Accuracy: 97.75%\n",
            "Mean Accuracy: 97.91%\n",
            "Standard Deviation: 0.0010\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from statistics import mean, stdev\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = cicids2017_df.drop('Attack Type', axis=1)\n",
        "y = cicids2017_df['Attack Type']\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1) # Define a StratifiedKFold object 'skf' that will do 5-fold cross-validation\n",
        "accuracy_scores = [] # Create a list to keep track of accuraccies\n",
        "\n",
        "splits = list(skf.split(X, y)) # Generate a list of the indices for each fold. Each element of the splits list is a (train_index, test_index) tuple for a fold.\n",
        "\n",
        "for fold in range(len(splits)): # Start folding\n",
        "\n",
        "    train_index, test_index = splits[fold] # Unpack the current indieces fold from splits and place them in train_index, test_index. Eg: if splits[fold] = (array([0,1,2,4]), array([3,5])) then train_index = array([0,1,2,4]) & test_index = array([3,5])\n",
        "\n",
        "    # Sanity check\n",
        "    print(f\"Fold {fold+1}:\")\n",
        "    print(f\"Train_index: {train_index}\")\n",
        "    print(f\"Test_index: {test_index}\")\n",
        "\n",
        "\n",
        "    # Split data into the training and testing folds (use iloc since the dataframe is still a panda dataframw)\n",
        "    x_train_fold = X.iloc[train_index]  # Feature rows for training\n",
        "    y_train_fold = y.iloc[train_index]  # Label rows for training\n",
        "    x_test_fold = X.iloc[test_index]    # Feature rows for testing\n",
        "    y_test_fold = y.iloc[test_index]    # Label rows for testing\n",
        "\n",
        "    # Standarise\n",
        "    x_train_fold = scaler.fit_transform(x_train_fold) # Standardise the folds according to the test fold\n",
        "    x_test_fold = scaler.transform(x_test_fold)\n",
        "\n",
        "    # Shuffle the data\n",
        "    x_train_fold, y_train_fold = shuffle(x_train_fold, y_train_fold, random_state=42)\n",
        "    x_test_fold, y_test_fold = shuffle(x_test_fold, y_test_fold, random_state=42)\n",
        "\n",
        "    # Shape the data for CNN Input (A 1D Convolutional Neural Network (Conv1D) expects 3D input:(samples, timesteps, channels))\n",
        "    x_train_fold = x_train_fold.reshape((x_train_fold.shape[0], x_train_fold.shape[1], 1)) # The actual 3D training array fed into the CNN for training, therefore the shape must be (samples, timesteps, 1)\n",
        "    x_test_fold = x_test_fold.reshape((x_test_fold.shape[0], x_test_fold.shape[1], 1))\n",
        "\n",
        "    input_shape = (x_train_fold.shape[1], 1) # The shape of one input sample, which is fed to the CNN layer, therefore shape: (timesteps, channels)\n",
        "    num_classes = len(attack_type_map)\n",
        "\n",
        "    # Sanity check\n",
        "    print(f\"Training fold input shape: {x_train_fold.shape}\")\n",
        "    print(f\"Testing fold input shape: {x_test_fold.shape}\")\n",
        "    print(f\"Single sample's input shape: {input_shape}\")\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Define the Baseline CNN model\n",
        "    baseline_model = Sequential([\n",
        "      Input(shape=input_shape),\n",
        "\n",
        "      Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
        "      BatchNormalization(),\n",
        "      MaxPooling1D(pool_size=2),\n",
        "\n",
        "      Conv1D(filters=16, kernel_size=3, activation='relu'),\n",
        "      BatchNormalization(),\n",
        "      MaxPooling1D(pool_size=2),\n",
        "\n",
        "      Flatten(),\n",
        "      Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0)),\n",
        "      Dropout(0.5),\n",
        "      Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    baseline_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                   patience=3,\n",
        "                                   restore_best_weights=True)\n",
        "\n",
        "    # Train the baseline model\n",
        "    history = baseline_model.fit(\n",
        "        x_train_fold, y_train_fold,\n",
        "        epochs=5,\n",
        "        batch_size=64,\n",
        "        validation_split=0.1,   # Reserve 10% of training for validation\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate the baseline model\n",
        "    y_pred_probs = baseline_model.predict(x_test_fold,verbose=1)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_test_fold, y_pred)\n",
        "    print(f\"Fold {fold+1} Accuracy: {acc:.4f}\")\n",
        "    accuracy_scores.append(acc)\n",
        "\n",
        "    cm = confusion_matrix(y_test_fold, y_pred)\n",
        "    cm_report =  classification_report(y_test_fold, y_pred)\n",
        "    print(f\"Fold {fold+1} Confusion Matric report:\\n{cm_report}\")\n",
        "\n",
        "\n",
        "# Final results\n",
        "print('\\nList of accuracy scores:', accuracy_scores)\n",
        "print('Maximum Accuracy: {:.2f}%'.format(max(accuracy_scores) * 100))\n",
        "print('Minimum Accuracy: {:.2f}%'.format(min(accuracy_scores) * 100))\n",
        "print('Mean Accuracy: {:.2f}%'.format(mean(accuracy_scores) * 100))\n",
        "print('Standard Deviation: {:.4f}'.format(stdev(accuracy_scores)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gteTlFJQjAc6"
      },
      "source": [
        "# PCA-CNN K-FOLD CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tO-dPnrtjEyN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "9d6d4607-4d4f-4e19-90f2-37fd0ee5ddd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1:\n",
            "Train_index: [      0       1       3 ... 4397845 4397847 4397848]\n",
            "Test_index: [      2      11      14 ... 4397841 4397842 4397846]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3910323444.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Standarise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mx_train_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_fold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Standardise the folds according to the test fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mx_test_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m                 self.mean_, self.var_, self.n_samples_seen_ = _incremental_mean_and_var(\n\u001b[0m\u001b[1;32m   1017\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36m_incremental_mean_and_var\u001b[0;34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             )\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mcorrection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[0;34m**=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0mnew_unnormalized_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36m_safe_accumulator_op\u001b[0;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;31m# see https://github.com/numpy/numpy/issues/9393. The float64 is also retained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;31m# as it is in case the float overflows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m     \"\"\"\n\u001b[1;32m    993\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mprovides\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0maccumulator\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat64\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from statistics import mean, stdev\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = cicids2017_df.drop('Attack Type', axis=1)\n",
        "y = cicids2017_df['Attack Type']\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1) # Define a StratifiedKFold object 'skf' that will do 5-fold cross-validation\n",
        "accuracy_scores = [] # Create a list to keep track of accuraccies\n",
        "\n",
        "splits = list(skf.split(X, y)) # Generate a list of the indices for each fold. Each element of the splits list is a (train_index, test_index) tuple for a fold.\n",
        "\n",
        "for fold in range(len(splits)): # Start folding\n",
        "\n",
        "    train_index, test_index = splits[fold] # Unpack the current indieces fold from splits and place them in train_index, test_index. Eg: if splits[fold] = (array([0,1,2,4]), array([3,5])) then train_index = array([0,1,2,4]) & test_index = array([3,5])\n",
        "\n",
        "    # Sanity check\n",
        "    print(f\"Fold {fold+1}:\")\n",
        "    print(f\"Train_index: {train_index}\")\n",
        "    print(f\"Test_index: {test_index}\")\n",
        "\n",
        "\n",
        "    # Split data into the training and testing folds (use iloc since the dataframe is still a panda dataframw)\n",
        "    x_train_fold = X.iloc[train_index]  # Feature rows for training\n",
        "    y_train_fold = y.iloc[train_index]  # Label rows for training\n",
        "    x_test_fold = X.iloc[test_index]    # Feature rows for testing\n",
        "    y_test_fold = y.iloc[test_index]    # Label rows for testing\n",
        "\n",
        "    # Standarise\n",
        "    x_train_fold = scaler.fit_transform(x_train_fold) # Standardise the folds according to the test fold\n",
        "    x_test_fold = scaler.transform(x_test_fold)\n",
        "\n",
        "    # Apply PCA\n",
        "    pca = PCA(n_components = 25)\n",
        "    x_train_fold = pca.fit_transform(x_train_fold)\n",
        "    x_test_fold = pca.transform(x_test_fold)\n",
        "\n",
        "    # Shuffle the data\n",
        "    x_train_fold, y_train_fold = shuffle(x_train_fold, y_train_fold, random_state=42)\n",
        "    x_test_fold, y_test_fold = shuffle(x_test_fold, y_test_fold, random_state=42)\n",
        "\n",
        "    # Shape the data for CNN Input (A 1D Convolutional Neural Network (Conv1D) expects 3D input:(samples, timesteps, channels))\n",
        "    x_train_fold = x_train_fold.reshape((x_train_fold.shape[0], x_train_fold.shape[1], 1)) # The actual 3D training array fed into the CNN for training, therefore the shape must be (samples, timesteps, 1)\n",
        "    x_test_fold = x_test_fold.reshape((x_test_fold.shape[0], x_test_fold.shape[1], 1))\n",
        "\n",
        "    input_shape = (x_train_fold.shape[1], 1) # The shape of one input sample, which is fed to the CNN layer, therefore shape: (timesteps, channels)\n",
        "    num_classes = len(attack_type_map)\n",
        "\n",
        "    # Sanity check\n",
        "    print(f\"Training fold input shape: {x_train_fold.shape}\")\n",
        "    print(f\"Testing fold input shape: {x_test_fold.shape}\")\n",
        "    print(f\"Single sample's input shape: {input_shape.shape}\")\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Define the Baseline CNN model\n",
        "    cnn_model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "\n",
        "        Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "\n",
        "        Conv1D(filters=16, kernel_size=3, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0)),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    cnn_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                   patience=3,\n",
        "                                   restore_best_weights=True)\n",
        "\n",
        "    # Train the baseline model\n",
        "    history = cnn_model.fit(\n",
        "        x_train_fold, y_train_fold,\n",
        "        epochs=5,\n",
        "        batch_size=64,\n",
        "        validation_split=0.1,   # Reserve 10% of training for validation\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate the baseline model\n",
        "    y_pred_probs = cnn_model.predict(x_test_fold)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_test_fold, y_pred)\n",
        "    print(f\"Fold {fold+1} Accuracy: {acc:.4f}\")\n",
        "    accuracy_scores.append(acc)\n",
        "\n",
        "    cm = confusion_matrix(y_test_fold, y_pred)\n",
        "    cm_report =  classification_report(y_test_fold, y_pred)\n",
        "    print(f\"Fold {fold+1} Confusion Matric report:\\n{cm_report}\")\n",
        "\n",
        "\n",
        "# Final results\n",
        "print('\\nList of accuracy scores:', accuracy_scores)\n",
        "print('Maximum Accuracy: {:.2f}%'.format(max(accuracy_scores) * 100))\n",
        "print('Minimum Accuracy: {:.2f}%'.format(min(accuracy_scores) * 100))\n",
        "print('Mean Accuracy: {:.2f}%'.format(mean(accuracy_scores) * 100))\n",
        "print('Standard Deviation: {:.4f}'.format(stdev(accuracy_scores)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rHxrJoAoPBV"
      },
      "source": [
        "# AE K_FOLD CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4LxBGY-oUIG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAutvewaplQN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from statistics import mean, stdev\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# --- Separate Benign and Malicious Traffic ---\n",
        "\n",
        "# Separate benign and malicious samples\n",
        "benign_df = cicids2017_df[cicids2017_df['Attack Type'] == 0]\n",
        "malicious_df = cicids2017_df[cicids2017_df['Attack Type'] != 0]\n",
        "\n",
        "# Separate features (X) and labels (y) for benign data\n",
        "X_benign = benign_df.drop('Attack Type', axis=1)\n",
        "y_benign = benign_df['Attack Type']\n",
        "\n",
        "# Separate features (X) and labels (y) for malicious data\n",
        "X_malicious = malicious_df.drop('Attack Type', axis=1)\n",
        "y_malicious = malicious_df['Attack Type']\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1) # Define a StratifiedKFold object 'skf' that will do 5-fold cross-validation\n",
        "accuracy_scores = [] # Create a list to keep track of accuraccies\n",
        "\n",
        "splits = list(skf.split(X_benign, y_benign)) # Generate a list of the indices for each fold. Each element of the splits list is a (train_index, test_index) tuple for a fold.\n",
        "\n",
        "for fold in range(len(splits)): # Start folding\n",
        "\n",
        "    train_index, test_index = splits[fold] # Unpack the current indieces fold from splits and place them in train_index, test_index. Eg: if splits[fold] = (array([0,1,2,4]), array([3,5])) then train_index = array([0,1,2,4]) & test_index = array([3,5])\n",
        "\n",
        "    # Sanity check\n",
        "    print(f\"Fold {fold+1}:\")\n",
        "\n",
        "    # Split data into the training and testing folds (use iloc since the dataframe is still a panda dataframe)\n",
        "    x_train_fold = X.iloc[train_index]  # Feature rows for training\n",
        "    y_train_fold = y.iloc[train_index]  # Label rows for training\n",
        "    x_test_fold = X.iloc[test_index]    # Feature rows for testing\n",
        "    y_test_fold = y.iloc[test_index]    # Label rows for testing\n",
        "\n",
        "    # Concatenate benign and malicious data for classifier and test sets\n",
        "    x_test_fold = pd.concat([x_test_fold, X_malicious], ignore_index=True)\n",
        "    y_test_fold = pd.concat([y_test_fold, y_malicious], ignore_index=True)\n",
        "\n",
        "    # Shuffle the data\n",
        "    x_train_fold, y_train_fold = shuffle(x_train_fold, y_train_fold, random_state=42)\n",
        "    x_test_fold, y_test_fold = shuffle(x_test_fold, y_test_fold, random_state=42)\n",
        "\n",
        "    # Reset indexes\n",
        "    x_test_fold = x_test_fold.reset_index(drop=True)\n",
        "    y_test_fold = y_test_fold.reset_index(drop=True)\n",
        "\n",
        "    # Normalise data\n",
        "    x_train_fold = scaler.fit_transform(x_train_fold)\n",
        "    x_test_fold = scaler.transform(x_test_fold)\n",
        "\n",
        "    # Define the Baseline CNN model\n",
        "    autoencoder_input_dim = x_train_fold.shape[1]\n",
        "\n",
        "    ae_model = Sequential([\n",
        "        Input(shape=(autoencoder_input_dim,)),\n",
        "        Dense(64, activation='relu'),   # Encoder layer\n",
        "        Dense(32, activation='relu'),   # Encoder layer\n",
        "        Dense(16, activation='relu'),   # Encoder layer\n",
        "        Dense(8, activation='relu'),    # Encoder layer\n",
        "        Dense(4, activation='relu'),    # Bottleneck layer\n",
        "        Dense(8, activation='relu'),    # Decoder layer\n",
        "        Dense(16, activation='relu'),   # Decoder layer\n",
        "        Dense(32, activation='relu'),   # Decoder layer\n",
        "        Dense(64, activation='relu'),   # Decoder layers\n",
        "        Dense(autoencoder_input_dim, activation='sigmoid')  # Output layer sigmoid\n",
        "])\n",
        "\n",
        "    ae_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                   patience=3,\n",
        "                                   restore_best_weights=True)\n",
        "\n",
        "    # Train the baseline model\n",
        "    history = ae_model.fit(\n",
        "        x_train_fold, y_train_fold,\n",
        "        epochs=5,\n",
        "        batch_size=64,\n",
        "        validation_split=0.1,   # Reserve 10% of training for validation\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Get threshold\n",
        "    y_pred_probs = ae_model.predict(x_train_fold)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    train_reconstructions = ae_model.predict(x_train_fold, verbose=0) #try reconstruct validation dataset\n",
        "    train_reconstruction_errors = np.abs(train_reconstructions - x_train_fold) # get the Mean Absolute Error (MAE) per feature, per sample\n",
        "\n",
        "    # Mean per feature thresholds ---\n",
        "    mean_feature_errors = np.mean(train_reconstruction_errors, axis=0)  # mean reconstruction error per feature. axis = 0 because we're averaging over all samples for each feature.\n",
        "    std_feature_errors = np.std(train_reconstruction_errors, axis=0)    # std of reconstruction error per feature\n",
        "\n",
        "    pre_feature_thresholds = mean_feature_errors + std_feature_errors\n",
        "\n",
        "    # Evaluate AE\n",
        "    test_reconstructions = ae_model.predict(x_test_fold, verbose=1)\n",
        "    test_reconstruction_errors = np.abs(test_reconstructions - x_test_fold)\n",
        "    AE_y_pred = (test_reconstruction_errors > pre_feature_thresholds).any(axis=1).astype(int)\n",
        "\n",
        "    y_test_binary = (x_test_fold != 0).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test_binary, AE_y_pred)\n",
        "    print(f\"Fold {fold+1} Accuracy: {acc:.4f}\")\n",
        "    accuracy_scores.append(acc)\n",
        "\n",
        "    cm = confusion_matrix(y_test_binary, AE_y_pred)\n",
        "    cm_report =  classification_report(y_test_binary, AE_y_pred)\n",
        "    print(f\"Fold {fold+1} Confusion Matric report:\\n{cm_report}\")\n",
        "\n",
        "\n",
        "# Final results\n",
        "print('\\nList of accuracy scores:', accuracy_scores)\n",
        "print('Maximum Accuracy: {:.2f}%'.format(max(accuracy_scores) * 100))\n",
        "print('Minimum Accuracy: {:.2f}%'.format(min(accuracy_scores) * 100))\n",
        "print('Mean Accuracy: {:.2f}%'.format(mean(accuracy_scores) * 100))\n",
        "print('Standard Deviation: {:.4f}'.format(stdev(accuracy_scores)))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}