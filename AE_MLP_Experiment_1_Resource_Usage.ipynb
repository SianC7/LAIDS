{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1jnR0fCVZYacXkc1a4dzdScbESqjNjGYC",
      "authorship_tag": "ABX9TyPCw95neorrOYKoFkx0LbaJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SianC7/LAIDS/blob/main/AE_MLP_Experiment_1_Resource_Usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Training & Test Data"
      ],
      "metadata": {
        "id": "uqPvnsxIZbFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- Data Collection ---\n",
        "\n",
        "# Set pandas display options for wide output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/Colab Notebooks/Honours Project/Sian's Models/AE-MLP files\"\n",
        "\n",
        "# Training sets\n",
        "X_train = pd.read_csv(os.path.join(save_path, \"AE_X_train.csv\")).to_numpy()\n",
        "y_train = pd.read_csv(os.path.join(save_path, \"AE_y_train.csv\")).to_numpy().ravel()\n",
        "\n",
        "# Validation sets\n",
        "X_val = pd.read_csv(os.path.join(save_path, \"AE_X_val.csv\")).to_numpy()\n",
        "y_val = pd.read_csv(os.path.join(save_path, \"AE_y_val.csv\")).to_numpy().ravel()\n",
        "\n",
        "# Classifier training sets\n",
        "X_classifier_train = pd.read_csv(os.path.join(save_path, \"adasyn_classifier_X_train.csv\")).to_numpy() # MLP training data augumented using adasyn\n",
        "y_classifier_train = pd.read_csv(os.path.join(save_path, \"adasyn_classifier_y_train.csv\")).to_numpy().ravel()\n",
        "\n",
        "# Classifier validation sets\n",
        "X_classifier_val = pd.read_csv(os.path.join(save_path, \"classifier_X_val.csv\")).to_numpy()\n",
        "y_classifier_val = pd.read_csv(os.path.join(save_path, \"classifier_y_val.csv\")).to_numpy().ravel()\n",
        "\n",
        "# Test sets\n",
        "X_test = pd.read_csv(os.path.join(save_path, \"X_test.csv\")).to_numpy()\n",
        "y_test = pd.read_csv(os.path.join(save_path, \"y_test.csv\")).to_numpy().ravel()\n",
        "\n",
        "\n",
        "# Print class distribution for each set\n",
        "print(\"\\nAE Training class distribution:\")\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "print(\"\\nAE Validation class distribution:\")\n",
        "unique, counts = np.unique(y_val, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "print(\"\\nClassfier Training class distribution:\")\n",
        "unique, counts = np.unique(y_classifier_train, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "print(\"\\nClassifier Validation class distribution:\")\n",
        "unique, counts = np.unique(y_classifier_val, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "print(\"\\nTest class distribution:\")\n",
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "print(dict(zip(unique, counts)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b1doEX5Zfuy",
        "outputId": "dd64e83f-e8ff-4e6e-9f38-96fa0f9d7d61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AE Training class distribution:\n",
            "{np.int64(0): np.int64(1357596)}\n",
            "\n",
            "AE Validation class distribution:\n",
            "{np.int64(0): np.int64(150844)}\n",
            "\n",
            "Classfier Training class distribution:\n",
            "{np.int64(0): np.int64(320544), np.int64(1): np.int64(286765), np.int64(2): np.int64(288505), np.int64(3): np.int64(288493), np.int64(4): np.int64(288577), np.int64(5): np.int64(288455), np.int64(6): np.int64(288550)}\n",
            "\n",
            "Classifier Validation class distribution:\n",
            "{np.int64(0): np.int64(56567), np.int64(1): np.int64(10883), np.int64(2): np.int64(257), np.int64(3): np.int64(1098), np.int64(4): np.int64(15362), np.int64(5): np.int64(234), np.int64(6): np.int64(23250)}\n",
            "\n",
            "Test class distribution:\n",
            "{np.int64(0): np.int64(209506), np.int64(1): np.int64(18139), np.int64(2): np.int64(429), np.int64(3): np.int64(1830), np.int64(4): np.int64(25603), np.int64(5): np.int64(389), np.int64(6): np.int64(38749)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the labels"
      ],
      "metadata": {
        "id": "Jwg92s-WH-ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Label Encoding ---\n",
        "\n",
        "# Create a mapping from attack type to integer label\n",
        "attack_type_map = {'Normal Traffic': 0, 'Port Scanning': 1, 'Web Attacks': 2, 'Brute Force': 3, 'DDoS': 4, 'Bots': 5, 'DoS': 6} # Use the specified mapping\n"
      ],
      "metadata": {
        "id": "WzilWwIYICRy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure datasets are numpy arrays for future operations"
      ],
      "metadata": {
        "id": "KhGupW-dPuF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all datasets\n",
        "datasets = {\n",
        "    \"X_train\": X_train,\n",
        "    \"y_train\": y_train,\n",
        "    \"X_val\": X_val,\n",
        "    \"y_val\": y_val,\n",
        "    \"X_classifier_train\": X_classifier_train,\n",
        "    \"y_classifier_train\": y_classifier_train,\n",
        "    \"X_classifier_val\": X_classifier_val,\n",
        "    \"y_classifier_val\": y_classifier_val,\n",
        "    \"X_test\": X_test,\n",
        "    \"y_test\": y_test\n",
        "}\n",
        "\n",
        "# Ensure all datasets are NumPy arrays\n",
        "for name, data in datasets.items():\n",
        "    if hasattr(data, \"to_numpy\"):\n",
        "        datasets[name] = data.to_numpy().ravel() if \"y_\" in name else data.to_numpy()\n",
        "\n",
        "# Update variables\n",
        "X_train = datasets[\"X_train\"]\n",
        "y_train = datasets[\"y_train\"]\n",
        "X_val = datasets[\"X_val\"]\n",
        "y_val = datasets[\"y_val\"]\n",
        "X_classifier_train = datasets[\"X_classifier_train\"]\n",
        "y_classifier_train = datasets[\"y_classifier_train\"]\n",
        "X_classifier_val = datasets[\"X_classifier_val\"]\n",
        "y_classifier_val = datasets[\"y_classifier_val\"]\n",
        "X_test = datasets[\"X_test\"]\n",
        "y_test = datasets[\"y_test\"]"
      ],
      "metadata": {
        "id": "7Kdc-LJBQjsr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set OS Constraints"
      ],
      "metadata": {
        "id": "Pqmc4TasVi9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_NUM_INTRAOP_THREADS'] = '4'\n",
        "os.environ['TF_NUM_INTEROP_THREADS'] = '1'\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "os.environ['MKL_NUM_THREADS'] = '4'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
      ],
      "metadata": {
        "id": "hp0k39XOl1TI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load in Models"
      ],
      "metadata": {
        "id": "GqzPUDKR-Wfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# --- Load Keras Models ---\n",
        "# -------------------------\n",
        "ae_model_path = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Sian\\'s Models/AE-MLP files/Best_AE.keras'\n",
        "mlp_model_path = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Sian\\'s Models/AE-MLP files/Best_MLP.keras'\n",
        "\n",
        "ae_model = load_model(ae_model_path)\n",
        "mlp_model = load_model(mlp_model_path)\n",
        "\n",
        "ae_model_name_prefix = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Sian\\'s Models/AE-MLP files/AE'\n",
        "mlp_model_name_prefix = '/content/drive/MyDrive/Colab Notebooks/Honours Project/Sian\\'s Models/AE-MLP files/MLP'\n",
        "\n",
        "# -------------------------\n",
        "# --- Load Per-Feature Thresholds ---\n",
        "# -------------------------\n",
        "per_feature_thresholds_path = \"/content/drive/MyDrive/Colab Notebooks/Honours Project/Sian's Models/AE-MLP files/ae_per_feature_thresholds.npy\"\n",
        "per_feature_thresholds = np.load(per_feature_thresholds_path)\n",
        "\n",
        "# -------------------------\n",
        "# --- Load TFLite Models ---\n",
        "# -------------------------\n",
        "def load_tflite_model(path):\n",
        "    \"\"\"Load TFLite model as bytes.\"\"\"\n",
        "    with open(path, 'rb') as f:\n",
        "        return f.read()\n",
        "\n",
        "# Base paths for saved TFLite models\n",
        "ae_model_base = ae_model_name_prefix\n",
        "mlp_model_base = mlp_model_name_prefix\n",
        "\n",
        "# List of TFLite models (AE → MLP)\n",
        "tflite_models = [\n",
        "    {\n",
        "        \"ae_model\": load_tflite_model(ae_model_base + '_float32.tflite'),\n",
        "        \"ae_name\": \"AE Float32 Model\",\n",
        "        \"ae_filename\": \"_float32.tflite\",\n",
        "        \"mlp_model\": load_tflite_model(mlp_model_base + '_float32.tflite'),\n",
        "        \"mlp_name\": \"MLP Float32 Model\",\n",
        "        \"mlp_filename\": \"_float32.tflite\"\n",
        "    },\n",
        "    {\n",
        "        \"ae_model\": load_tflite_model(ae_model_base + '_fp16_weights.tflite'),\n",
        "        \"ae_name\": \"AE Float16 Weights-Only Model\",\n",
        "        \"ae_filename\": \"_fp16_weights.tflite\",\n",
        "        \"mlp_model\": load_tflite_model(mlp_model_base + '_fp16_weights.tflite'),\n",
        "        \"mlp_name\": \"MLP Float16 Weights-Only Model\",\n",
        "        \"mlp_filename\": \"_fp16_weights.tflite\"\n",
        "    },\n",
        "    {\n",
        "        \"ae_model\": load_tflite_model(ae_model_base + '_int8_weights.tflite'),\n",
        "        \"ae_name\": \"AE Int8 Weights-Only Model\",\n",
        "        \"ae_filename\": \"_int8_weights.tflite\",\n",
        "        \"mlp_model\": load_tflite_model(mlp_model_base + '_int8_weights.tflite'),\n",
        "        \"mlp_name\": \"MLP Int8 Weights-Only Model\",\n",
        "        \"mlp_filename\": \"_int8_weights.tflite\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"All Keras and TFLite models loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "hiwxbSLk-You",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de2185a-322a-4903-d7a1-1797a443e69c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Keras and TFLite models loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resource Evaluation of Dynamic Range Quantization (Weight-only quantisation)"
      ],
      "metadata": {
        "id": "mZZHnZoF-vx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "D2NihRdLm-oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import psutil\n",
        "import time\n",
        "import resource\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# --- Imports ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "import os"
      ],
      "metadata": {
        "id": "Hprzx6shm_js"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set resource constraints"
      ],
      "metadata": {
        "id": "7RvkTflk3B2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab Resource Configuration\n",
        "\n",
        "\n",
        "def configure_resources_for_colab():\n",
        "    print(\"🔧 Configuring resources for Google Colab...\")\n",
        "\n",
        "    # 1. CPU Core Limitation (4 cores max)\n",
        "    os.environ['OMP_NUM_THREADS'] = '4'\n",
        "    os.environ['MKL_NUM_THREADS'] = '4'\n",
        "    os.environ['NUMEXPR_NUM_THREADS'] = '4'\n",
        "    os.environ['OPENBLAS_NUM_THREADS'] = '4'\n",
        "\n",
        "    # 2. Set CPU affinity (works in Colab Linux environment)\n",
        "    try:\n",
        "        available_cores = min(4, os.cpu_count())\n",
        "        os.sched_setaffinity(0, list(range(available_cores)))\n",
        "        print(f\"✅ CPU affinity set to {available_cores} cores\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ CPU affinity warning: {e}\")\n",
        "\n",
        "    # 3. Memory limitation (4GB)\n",
        "    try:\n",
        "        memory_limit = 4 * 1024 * 1024 * 1024  # 4GB in bytes\n",
        "        resource.setrlimit(resource.RLIMIT_AS, (memory_limit, memory_limit))\n",
        "        print(f\"✅ Memory limit set to 4GB\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Memory limit warning: {e}\")\n",
        "\n",
        "    # 4. CPU frequency monitoring (informational in Colab)\n",
        "    try:\n",
        "        cpu_info = psutil.cpu_freq()\n",
        "        if cpu_info:\n",
        "            print(f\"ℹ️  CPU frequency: {cpu_info.current:.0f}MHz\")\n",
        "        else:\n",
        "            print(f\"ℹ️  CPU frequency info not available\")\n",
        "    except:\n",
        "        print(f\"ℹ️  Running on Google Colab - CPU frequency managed by platform\")\n",
        "\n",
        "    # 5. Process priority adjustment\n",
        "    try:\n",
        "        os.nice(5)  # Lower priority (0 is normal, positive is lower priority)\n",
        "        print(f\"✅ Process priority lowered\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Priority adjustment warning: {e}\")\n",
        "\n",
        "    print(\"✅ Resource configuration complete for Colab environment\")\n",
        "\n",
        "# Call the function\n",
        "configure_resources_for_colab()\n",
        "\n",
        "# TensorFlow configuration (enhanced for Colab)\n",
        "def configure_tensorflow_colab():\n",
        "    try:\n",
        "        # Try to limit CPU threads (only works before TF initialization)\n",
        "        tf.config.threading.set_intra_op_parallelism_threads(4)\n",
        "        tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "        print(\"✅ TensorFlow CPU threads configured\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"⚠️ TensorFlow threading config: {str(e)}\")\n",
        "        print(\"ℹ️  TensorFlow was already initialized - CPU thread limits may not be applied\")\n",
        "\n",
        "    # GPU configuration (Colab often has GPU available)\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            # Enable memory growth\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(f\"✅ GPU memory growth enabled\")\n",
        "\n",
        "            # Optional: Set GPU memory limit (may fail if already configured)\n",
        "            try:\n",
        "                tf.config.experimental.set_virtual_device_configuration(\n",
        "                    gpus[0],\n",
        "                    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]  # 4GB GPU limit\n",
        "                )\n",
        "                print(f\"✅ GPU memory limit set to 4GB\")\n",
        "            except RuntimeError as gpu_limit_error:\n",
        "                print(f\"ℹ️  GPU memory limit not set: {str(gpu_limit_error)}\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"⚠️ GPU configuration: {e}\")\n",
        "    else:\n",
        "        print(f\"ℹ️  No GPU detected - using CPU only\")\n",
        "\n",
        "    # Reduce TensorFlow verbosity\n",
        "    tf.get_logger().setLevel('WARNING')\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "    print(\"✅ TensorFlow configuration complete\")\n",
        "\n",
        "configure_tensorflow_colab()\n",
        "\n",
        "# Resource monitoring for Colab\n",
        "def monitor_colab_resources():\n",
        "    try:\n",
        "        # Memory usage\n",
        "        mem = psutil.virtual_memory()\n",
        "        print(f\"📊 Memory: {mem.used / (1024**3):.1f}GB used / {mem.total / (1024**3):.1f}GB total ({mem.percent:.1f}%)\")\n",
        "\n",
        "        # CPU usage\n",
        "        cpu_percent = psutil.cpu_percent(interval=1)\n",
        "        print(f\"📊 CPU usage: {cpu_percent:.1f}%\")\n",
        "\n",
        "        # Disk space\n",
        "        disk = psutil.disk_usage('/')\n",
        "        print(f\"📊 Disk: {disk.used / (1024**3):.1f}GB used / {disk.total/(1024**3):.1f}GB total ({disk.percent:.1f}%)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Monitoring error: {e}\")\n",
        "\n",
        "monitor_colab_resources()"
      ],
      "metadata": {
        "id": "uirBBsWgZB3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fac2d07-e6eb-4a7e-f662-57370446cb75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Configuring resources for Google Colab...\n",
            "✅ CPU affinity set to 2 cores\n",
            "✅ Memory limit set to 4GB\n",
            "ℹ️  CPU frequency: 2000MHz\n",
            "✅ Process priority lowered\n",
            "✅ Resource configuration complete for Colab environment\n",
            "⚠️ TensorFlow threading config: Intra op parallelism cannot be modified after initialization.\n",
            "ℹ️  TensorFlow was already initialized - CPU thread limits may not be applied\n",
            "⚠️ GPU configuration: Physical devices cannot be modified after being initialized\n",
            "✅ TensorFlow configuration complete\n",
            "📊 Memory: 3.0GB used / 12.7GB total (26.0%)\n",
            "📊 CPU usage: 2.0%\n",
            "📊 Disk: 41.7GB used / 112.6GB total (37.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resource usage evaluation"
      ],
      "metadata": {
        "id": "p8KozpUc213I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Storage for usage tracking\n",
        "global timestamps\n",
        "global cpu_usages\n",
        "global mem_usages\n",
        "timestamps = []\n",
        "cpu_usages = []\n",
        "mem_usages = []\n",
        "\n",
        "global process\n",
        "process = psutil.Process(os.getpid())\n",
        "global start_time_global\n",
        "\n",
        "def record_usage():\n",
        "    \"\"\"Capture CPU%, Memory, and elapsed inference time.\"\"\"\n",
        "    now = time.time() - start_time_global\n",
        "    cpu = process.cpu_percent(interval=None)  # non-blocking\n",
        "    mem = process.memory_info().rss / (1024**2)  # MB\n",
        "    timestamps.append(now)\n",
        "    cpu_usages.append(cpu)\n",
        "    mem_usages.append(mem)\n",
        "\n",
        "# ==========================\n",
        "# Evaluate each AE → MLP pair\n",
        "# ==========================\n",
        "for m in tflite_models:\n",
        "    timestamps = []\n",
        "    cpu_usages = []\n",
        "    mem_usages = []\n",
        "\n",
        "    ae_model_content = m[\"ae_model\"]\n",
        "    ae_name = m[\"ae_name\"]\n",
        "    ae_filename = m[\"ae_filename\"]\n",
        "\n",
        "    mlp_model_content = m[\"mlp_model\"]\n",
        "    mlp_name = m[\"mlp_name\"]\n",
        "    mlp_filename = m[\"mlp_filename\"]\n",
        "\n",
        "    print(f\"\\n--- Evaluating AE: {ae_name} → MLP: {mlp_name} ---\")\n",
        "\n",
        "    ae_file_path = os.path.join(os.path.dirname(ae_model_name_prefix),os.path.basename(ae_model_name_prefix) + ae_filename)\n",
        "    mlp_file_path = os.path.join(os.path.dirname(mlp_model_name_prefix),os.path.basename(mlp_model_name_prefix) + mlp_filename)\n",
        "    time.sleep(5)\n",
        "\n",
        "    # --- Get Models ready ---\n",
        "    # Autoencoder\n",
        "    ae_interpreter = tf.lite.Interpreter(model_content=ae_model_content)\n",
        "    ae_interpreter.allocate_tensors()\n",
        "    ae_input_details = ae_interpreter.get_input_details()\n",
        "    ae_output_details = ae_interpreter.get_output_details()\n",
        "\n",
        "    #MLP\n",
        "    mlp_interpreter = tf.lite.Interpreter(model_content=mlp_model_content)\n",
        "    mlp_interpreter.allocate_tensors()\n",
        "    mlp_input_details = mlp_interpreter.get_input_details()\n",
        "    mlp_output_details = mlp_interpreter.get_output_details()\n",
        "\n",
        "    # --- Start Autoencoder Resource Evaluation ---\n",
        "    start_time = time.time()\n",
        "    start_time_global = start_time\n",
        "\n",
        "    record_usage()\n",
        "    ae_y_pred_probs = []\n",
        "    record_usage()\n",
        "    for i in range(1):\n",
        "        input_data = np.expand_dims(X_test[i], axis=0).astype(ae_input_details[0]['dtype'])\n",
        "        ae_interpreter.set_tensor(ae_input_details[0]['index'], input_data)\n",
        "        ae_interpreter.invoke()\n",
        "        record_usage()\n",
        "        ae_output_data = ae_interpreter.get_tensor(ae_output_details[0]['index'])\n",
        "        ae_y_pred_probs.append(ae_output_data[0])\n",
        "        record_usage()\n",
        "\n",
        "    ae_y_pred_probs = np.array(ae_y_pred_probs)\n",
        "    test_reconstruction_errors = np.abs(ae_y_pred_probs - X_test[i])\n",
        "    ae_y_pred = (test_reconstruction_errors > per_feature_thresholds).any(axis=1).astype(int)\n",
        "    malicious_pred_indices = np.flatnonzero(ae_y_pred)\n",
        "    record_usage()\n",
        "\n",
        "    # --- Start MLP Resource Evaluation ---\n",
        "    if len(malicious_pred_indices) > 0:\n",
        "        mlp_X_test = X_test[malicious_pred_indices]\n",
        "        mlp_y_pred_probs = []\n",
        "        record_usage()\n",
        "        for i in range(1):\n",
        "            input_data = np.expand_dims(mlp_X_test[i], axis=0).astype(mlp_input_details[0]['dtype'])\n",
        "            mlp_interpreter.set_tensor(mlp_input_details[0]['index'], input_data)\n",
        "            mlp_interpreter.invoke()\n",
        "            record_usage()\n",
        "            mlp_output_data = mlp_interpreter.get_tensor(mlp_output_details[0]['index'])\n",
        "            mlp_y_pred_probs.append(mlp_output_data[0])\n",
        "            record_usage()\n",
        "\n",
        "        mlp_y_pred_probs = np.array(mlp_y_pred_probs)\n",
        "        mlp_y_pred = np.argmax(mlp_y_pred_probs, axis=1)\n",
        "        record_usage()\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # --- Final Measurement ---\n",
        "\n",
        "    total_storage_mb = (os.path.getsize(ae_file_path) + os.path.getsize(mlp_file_path)) / (1024**2)\n",
        "    inference = end_time - start_time\n",
        "    avg_cpu = np.mean(cpu_usages)\n",
        "    avg_mem = np.mean(mem_usages)\n",
        "    print(f\"\\n--- Resource Usage for {ae_name} → {mlp_name} ---\")\n",
        "    print(f\"Total storage size: {total_storage_mb} MB\")\n",
        "    print(f\"Inference time: {inference:.4f} sec\")\n",
        "    print(f\"Average CPU usage: {avg_cpu}%\")\n",
        "    print(f\"Average memory usage: {avg_mem} MB\")\n",
        "\n",
        "    # ==========================\n",
        "    # --- Plot CPU & RAM vs Time\n",
        "    # ==========================\n",
        "     # --- Save CPU & RAM Plots ---\n",
        "    # fig_filename = f\"{ae_name}_to_{mlp_name}_usage.png\".replace(\" \", \"_\")\n",
        "    # plt.figure(figsize=(12,6))\n",
        "    # plt.subplot(2,1,1)\n",
        "    # plt.plot(timestamps, cpu_usages, marker='o', label=\"CPU Usage (%)\")\n",
        "    # plt.ylabel(\"CPU %\")\n",
        "    # plt.legend()\n",
        "    # plt.subplot(2,1,2)\n",
        "    # plt.plot(timestamps, mem_usages, marker='o', color=\"orange\", label=\"RAM (MB)\")\n",
        "    # plt.xlabel(\"Inference Time (s)\")\n",
        "    # plt.ylabel(\"Memory (MB)\")\n",
        "    # plt.legend()\n",
        "    # plt.suptitle(f\"CPU & Memory Usage: {ae_name} → {mlp_name}\")\n",
        "    # plt.savefig(fig_filename, dpi=150, bbox_inches='tight')\n",
        "    # plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "3oi6-tnHE171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b627b7c-7ef0-49ab-dc5d-0759e806cf4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating AE: AE Float32 Model → MLP: MLP Float32 Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Resource Usage for AE Float32 Model → MLP Float32 Model ---\n",
            "Total storage size: 0.07562637329101562 MB\n",
            "Inference time: 0.0012 sec\n",
            "Average CPU usage: 0.0%\n",
            "Average memory usage: 2696.6796875 MB\n",
            "\n",
            "--- Evaluating AE: AE Float16 Weights-Only Model → MLP: MLP Float16 Weights-Only Model ---\n",
            "\n",
            "--- Resource Usage for AE Float16 Weights-Only Model → MLP Float16 Weights-Only Model ---\n",
            "Total storage size: 0.04409027099609375 MB\n",
            "Inference time: 0.0006 sec\n",
            "Average CPU usage: 0.04%\n",
            "Average memory usage: 2696.6796875 MB\n",
            "\n",
            "--- Evaluating AE: AE Int8 Weights-Only Model → MLP: MLP Int8 Weights-Only Model ---\n",
            "\n",
            "--- Resource Usage for AE Int8 Weights-Only Model → MLP Int8 Weights-Only Model ---\n",
            "Total storage size: 0.03321075439453125 MB\n",
            "Inference time: 0.0009 sec\n",
            "Average CPU usage: 0.0%\n",
            "Average memory usage: 2696.6796875 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full integer 8 Quantisation of the model"
      ],
      "metadata": {
        "id": "GMpJPw5lQ7jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# def ae_representative_dataset_gen():\n",
        "#     # Randomly select 1000 indices\n",
        "#     total_samples = len(X_train)\n",
        "#     selected_indices = np.random.choice(total_samples, 1000, replace=False)\n",
        "\n",
        "#     # Yield samples with shape suitable for a fully connected autoencoder: (1, feature_dim)\n",
        "#     for i in selected_indices:\n",
        "#         yield [X_train[i].astype(np.float32).reshape(1, -1)]\n"
      ],
      "metadata": {
        "id": "CXdH4miwUoOh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# def mlp_representative_dataset_gen():\n",
        "#     # Get class distribution\n",
        "#     class_types, class_counts = np.unique(y_classifier_train, return_counts=True)\n",
        "#     min_samples_per_class = min(class_counts)\n",
        "\n",
        "#     class_indices = [np.where(y_classifier_train == c)[0] for c in class_types]\n",
        "\n",
        "#     # Pick equal number of samples per class (e.g., 100 per class)\n",
        "#     selected_indices = []\n",
        "#     for indices in class_indices:\n",
        "#         selected_indices.extend(np.random.choice(indices, 100, replace=False))\n",
        "\n",
        "#     np.random.shuffle(selected_indices)\n",
        "#     representative_indices = selected_indices\n",
        "\n",
        "#     # Yield samples reshaped for MLP: (1, feature_dim)\n",
        "#     for i in representative_indices:\n",
        "#         yield [X_classifier_train[i].astype(np.float32).reshape(1, -1)]\n"
      ],
      "metadata": {
        "id": "qWFdkH2ENxWp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoder"
      ],
      "metadata": {
        "id": "Is1oHoa-oQYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- AE Full 8 Integer Model Quantization ---\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(ae_model)\n",
        "\n",
        "# # Create an int8 quantized model (requires representative dataset)\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT] # Apply default optimizations\n",
        "\n",
        "# #converter.target_spec.supported_types = [tf.int8] # Specify target data type as int8\n",
        "\n",
        "# converter.representative_dataset = ae_representative_dataset_gen # Provide the representative dataset and ensure input dtype is float32\n",
        "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]# Specify OpsSet must specifically targets the 8-bit integer quantized versions of the TensorFlow Lite built-in operations.\n",
        "\n",
        "# # Set the input and output types to int8 for inference\n",
        "# converter.inference_input_type = tf.int8\n",
        "# converter.inference_output_type = tf.int8\n",
        "# print(\"# --- AE Full 8 Integer Model Quantization ---\")\n",
        "# ae_quantModel_int8 = converter.convert() # Convert the model\n",
        "\n",
        "\n",
        "# # --- AE Perform 16x8 Full Integer Quantization ---\n",
        "# # This converts weights to int8 and activations to int16.\n",
        "# #converter = tf.lite.TFLiteConverter.from_keras_model(cnn_model)\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = ae_representative_dataset_gen\n",
        "# converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]# Specify the experimental ops set for 16x8 quantization - int16 activations and int8 weights.\n",
        "# converter.inference_input_type = tf.int16\n",
        "# converter.inference_output_type = tf.int16\n",
        "\n",
        "# print(\"# --- AE Perform 16x8 Full Integer Quantization ---\")\n",
        "# ae_quant_int16x8_model = converter.convert()# Convert the model\n",
        "\n",
        "\n",
        "# # --- Save models ---\n",
        "# os.makedirs('models', exist_ok=True)\n",
        "# ae_int8_full_path = ae_model_name_prefix + '_int8_full.tflite' # Define path\n",
        "# with open(ae_int8_full_path, 'wb') as f: # Added _full to filename\n",
        "#     f.write(ae_quantModel_int8)\n",
        "# print(f\"AE Int8 Full Integer model saved to: {os.path.abspath(ae_int8_full_path)}\") # Print path\n",
        "\n",
        "# ae_int16x8_full_path = ae_model_name_prefix + '_int16x8_full.tflite' # Define path\n",
        "# with open(ae_int16x8_full_path, 'wb') as f: # Added _full to filename\n",
        "#     f.write(ae_quant_int16x8_model)\n",
        "# print(f\"AE Int16x8 Full Integer model saved to: {os.path.abspath(ae_int16x8_full_path)}\") # Print path"
      ],
      "metadata": {
        "id": "cFISxYTDWD6J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "yEYXbf0noVb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- MLP Full 8 Integer Model Quantization ---\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(mlp_model) # Changed to mlp_model\n",
        "\n",
        "# # Create an int8 quantized model (requires representative dataset)\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT] # Apply default optimizations\n",
        "\n",
        "# converter.representative_dataset = mlp_representative_dataset_gen # Provide the representative dataset and ensure input dtype is float32\n",
        "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]# Specify OpsSet must specifically targets the 8-bit integer quantized versions of the TensorFlow Lite built-in operations.\n",
        "\n",
        "# # Set the input and output types to int8 for inference\n",
        "# converter.inference_input_type = tf.int8\n",
        "# converter.inference_output_type = tf.int8\n",
        "# print(\"# --- MLP Full 8 Integer Model Quantization ---\") # Changed to MLP\n",
        "# mlp_quantModel_int8 = converter.convert() # Changed to mlp_quantModel_int8\n",
        "\n",
        "\n",
        "# # --- MLP Perform 16x8 Full Integer Quantization --- # Changed to MLP\n",
        "# # This converts weights to int8 and activations to int16.\n",
        "# #converter = tf.lite.TFLiteConverter.from_keras_model(cnn_model) # This comment might be outdated, leaving as is.\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = mlp_representative_dataset_gen\n",
        "# converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]# Specify the experimental ops set for 16x8 quantization - int16 activations and int8 weights.\n",
        "# converter.inference_input_type = tf.int16\n",
        "# converter.inference_output_type = tf.int16\n",
        "\n",
        "# print(\"# --- MLP Perform 16x8 Full Integer Quantization ---\") # Changed to MLP\n",
        "# mlp_quant_int16x8_model = converter.convert()# Changed to mlp_quant_int16x8_model\n",
        "\n",
        "\n",
        "# # --- Save models ---\n",
        "# os.makedirs('models', exist_ok=True)\n",
        "# mlp_int8_full_path = mlp_model_name_prefix + '_int8_full.tflite' # Changed to mlp_model_name_prefix and mlp_int8_full_path\n",
        "# with open(mlp_int8_full_path, 'wb') as f:\n",
        "#     f.write(mlp_quantModel_int8) # Changed to mlp_quantModel_int8\n",
        "# print(f\"MLP Int8 Full Integer model saved to: {os.path.abspath(mlp_int8_full_path)}\") # Changed to MLP and mlp_int8_full_path\n",
        "\n",
        "# mlp_int16x8_full_path = mlp_model_name_prefix + '_int16x8_full.tflite' # Changed to mlp_model_name_prefix and mlp_int16x8_full_path\n",
        "# with open(mlp_int16x8_full_path, 'wb') as f:\n",
        "#     f.write(mlp_quant_int16x8_model) # Changed to mlp_quant_int16x8_model\n",
        "# print(f\"MLP Int16x8 Full Integer model saved to: {os.path.abspath(mlp_int16x8_full_path)}\") # Changed to MLP and mlp_int16x8_full_path"
      ],
      "metadata": {
        "id": "9Tb_6t46oW4a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of full int quant models"
      ],
      "metadata": {
        "id": "CLTfkBfzRPCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Google Colab Resource Configuration\n",
        "\n",
        "\n",
        "# def configure_resources_for_colab():\n",
        "#     print(\"🔧 Configuring resources for Google Colab...\")\n",
        "\n",
        "#     # 1. CPU Core Limitation (4 cores max)\n",
        "#     os.environ['OMP_NUM_THREADS'] = '4'\n",
        "#     os.environ['MKL_NUM_THREADS'] = '4'\n",
        "#     os.environ['NUMEXPR_NUM_THREADS'] = '4'\n",
        "#     os.environ['OPENBLAS_NUM_THREADS'] = '4'\n",
        "\n",
        "#     # 2. Set CPU affinity (works in Colab Linux environment)\n",
        "#     try:\n",
        "#         available_cores = min(4, os.cpu_count())\n",
        "#         os.sched_setaffinity(0, list(range(available_cores)))\n",
        "#         print(f\"✅ CPU affinity set to {available_cores} cores\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"⚠️ CPU affinity warning: {e}\")\n",
        "\n",
        "#     # 3. Memory limitation (4GB)\n",
        "#     try:\n",
        "#         memory_limit = 4 * 1024 * 1024 * 1024  # 4GB in bytes\n",
        "#         resource.setrlimit(resource.RLIMIT_AS, (memory_limit, memory_limit))\n",
        "#         print(f\"✅ Memory limit set to 4GB\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"⚠️ Memory limit warning: {e}\")\n",
        "\n",
        "#     # 4. CPU frequency monitoring (informational in Colab)\n",
        "#     try:\n",
        "#         cpu_info = psutil.cpu_freq()\n",
        "#         if cpu_info:\n",
        "#             print(f\"ℹ️  CPU frequency: {cpu_info.current:.0f}MHz\")\n",
        "#         else:\n",
        "#             print(f\"ℹ️  CPU frequency info not available\")\n",
        "#     except:\n",
        "#         print(f\"ℹ️  Running on Google Colab - CPU frequency managed by platform\")\n",
        "\n",
        "#     # 5. Process priority adjustment\n",
        "#     try:\n",
        "#         os.nice(5)  # Lower priority (0 is normal, positive is lower priority)\n",
        "#         print(f\"✅ Process priority lowered\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"⚠️ Priority adjustment warning: {e}\")\n",
        "\n",
        "#     print(\"✅ Resource configuration complete for Colab environment\")\n",
        "\n",
        "# # Call the function\n",
        "# configure_resources_for_colab()\n",
        "\n",
        "# # TensorFlow configuration (enhanced for Colab)\n",
        "# def configure_tensorflow_colab():\n",
        "#     try:\n",
        "#         # Try to limit CPU threads (only works before TF initialization)\n",
        "#         tf.config.threading.set_intra_op_parallelism_threads(4)\n",
        "#         tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "#         print(\"✅ TensorFlow CPU threads configured\")\n",
        "#     except RuntimeError as e:\n",
        "#         print(f\"⚠️ TensorFlow threading config: {str(e)}\")\n",
        "#         print(\"ℹ️  TensorFlow was already initialized - CPU thread limits may not be applied\")\n",
        "\n",
        "#     # GPU configuration (Colab often has GPU available)\n",
        "#     gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "#     if gpus:\n",
        "#         try:\n",
        "#             # Enable memory growth\n",
        "#             for gpu in gpus:\n",
        "#                 tf.config.experimental.set_memory_growth(gpu, True)\n",
        "#             print(f\"✅ GPU memory growth enabled\")\n",
        "\n",
        "#             # Optional: Set GPU memory limit (may fail if already configured)\n",
        "#             try:\n",
        "#                 tf.config.experimental.set_virtual_device_configuration(\n",
        "#                     gpus[0],\n",
        "#                     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]  # 4GB GPU limit\n",
        "#                 )\n",
        "#                 print(f\"✅ GPU memory limit set to 4GB\")\n",
        "#             except RuntimeError as gpu_limit_error:\n",
        "#                 print(f\"ℹ️  GPU memory limit not set: {str(gpu_limit_error)}\")\n",
        "\n",
        "#         except RuntimeError as e:\n",
        "#             print(f\"⚠️ GPU configuration: {e}\")\n",
        "#     else:\n",
        "#         print(f\"ℹ️  No GPU detected - using CPU only\")\n",
        "\n",
        "#     # Reduce TensorFlow verbosity\n",
        "#     tf.get_logger().setLevel('WARNING')\n",
        "#     os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "#     print(\"✅ TensorFlow configuration complete\")\n",
        "\n",
        "# configure_tensorflow_colab()\n",
        "\n",
        "# # Resource monitoring for Colab\n",
        "# def monitor_colab_resources():\n",
        "#     try:\n",
        "#         # Memory usage\n",
        "#         mem = psutil.virtual_memory()\n",
        "#         print(f\"📊 Memory: {mem.used / (1024**3):.1f}GB used / {mem.total / (1024**3):.1f}GB total ({mem.percent:.1f}%)\")\n",
        "\n",
        "#         # CPU usage\n",
        "#         cpu_percent = psutil.cpu_percent(interval=1)\n",
        "#         print(f\"📊 CPU usage: {cpu_percent:.1f}%\")\n",
        "\n",
        "#         # Disk space\n",
        "#         disk = psutil.disk_usage('/')\n",
        "#         print(f\"📊 Disk: {disk.used / (1024**3):.1f}GB used / {disk.total/(1024**3):.1f}GB total ({disk.percent:.1f}%)\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"⚠️ Monitoring error: {e}\")\n",
        "\n",
        "# monitor_colab_resources()"
      ],
      "metadata": {
        "id": "Se9fjjvMZRSD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # -- Imports ---\n",
        "# import os\n",
        "# import psutil\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "# import tensorflow as tf"
      ],
      "metadata": {
        "id": "8cfdXvP-WLxv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # List of TFLite models (AE -> MLP)\n",
        "# tflite_models = [\n",
        "#     {\n",
        "#         \"ae_model\": ae_quantModel_int8,\n",
        "#         \"ae_name\": \"AE Full Int8 Quantized Model\",\n",
        "#         \"ae_filename\": \"_int8_full.tflite\",\n",
        "#         \"mlp_model\": mlp_quantModel_int8,\n",
        "#         \"mlp_name\": \"MLP Full Int8 Quantized Model\",\n",
        "#         \"mlp_filename\": \"_int8_full.tflite\"\n",
        "#     },\n",
        "#     # {\n",
        "#     #     \"ae_model\": ae_quant_int16x8_model,\n",
        "#     #     \"ae_name\": \"AE Int16x8 Quantized Model\",\n",
        "#     #     \"ae_filename\": \"_int16x8_full.tflite\",\n",
        "#     #     \"mlp_model\": mlp_quant_int16x8_model,\n",
        "#     #     \"mlp_name\": \"MLP Int16x8 Quantized Model\",\n",
        "#     #     \"mlp_filename\": \"_int16x8_full.tflite\"\n",
        "#     # }\n",
        "# ]"
      ],
      "metadata": {
        "id": "bio27lwYDcq1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Evaluate all AE → MLP TFLite models ---\n",
        "# for m in tflite_models:\n",
        "#     # --- Unpack model info ---\n",
        "#     ae_model_content = m[\"ae_model\"]\n",
        "#     ae_name = m[\"ae_name\"]\n",
        "#     ae_filename = m[\"ae_filename\"]\n",
        "\n",
        "#     mlp_model_content = m[\"mlp_model\"]\n",
        "#     mlp_name = m[\"mlp_name\"]\n",
        "#     mlp_filename = m[\"mlp_filename\"]\n",
        "\n",
        "#     print(f\"\\n--- Evaluating AE: {ae_name} → MLP: {mlp_name} ---\")\n",
        "\n",
        "#     # --- Construct full file paths ---\n",
        "#     ae_file_path = os.path.join(os.path.dirname(ae_model_name_prefix),os.path.basename(ae_model_name_prefix) + ae_filename)\n",
        "#     mlp_file_path = os.path.join(os.path.dirname(mlp_model_name_prefix),os.path.basename(mlp_model_name_prefix) + mlp_filename)\n",
        "\n",
        "#     # --- Autoencoder ---\n",
        "#     ae_interpreter = tf.lite.Interpreter(model_content=ae_model_content)\n",
        "#     ae_interpreter.allocate_tensors()\n",
        "#     ae_input_details = ae_interpreter.get_input_details()[0]\n",
        "#     ae_output_details = ae_interpreter.get_output_details()[0]\n",
        "\n",
        "#     in_scale,  in_zp  = ae_input_details['quantization']\n",
        "#     out_scale, out_zp = ae_output_details['quantization']\n",
        "#     print(f\"AE Input Details: {ae_input_details}\")\n",
        "#     print(f\"AE Output Details: {ae_output_details}\")\n",
        "\n",
        "#     # Start resource tracking\n",
        "#     ae_process = psutil.Process(os.getpid())\n",
        "#     ae_mem_before = ae_process.memory_info().rss / (1024*1024)\n",
        "#     ae_cpu_before = psutil.cpu_percent(interval=None)\n",
        "#     ae_start_time = time.time()\n",
        "\n",
        "#    # --- Make new Thresholds for quantized model ---\n",
        "#     val_reconstructions = []\n",
        "#     # --- Get dequantized reconstruction for normal validation data ---\n",
        "#     for i in range(len(X_val)):\n",
        "#         x = np.expand_dims(X_val[i].astype(np.float32), axis=0)\n",
        "\n",
        "#         # Quantize input if necessary\n",
        "#         if ae_input_details['dtype'] == np.int8:\n",
        "#             xq = np.round(x / in_scale + in_zp).astype(np.int8)\n",
        "#         else:\n",
        "#             xq = x.astype(ae_input_details['dtype'])\n",
        "\n",
        "#         ae_interpreter.set_tensor(ae_input_details['index'], xq)\n",
        "#         ae_interpreter.invoke()\n",
        "#         yq = ae_interpreter.get_tensor(ae_output_details['index'])\n",
        "\n",
        "#         # Dequantize output to FP32\n",
        "#         if ae_output_details['dtype'] == np.int8:\n",
        "#             y = (yq.astype(np.float32) - out_zp) * out_scale\n",
        "#         else:\n",
        "#             y = yq.astype(np.float32)\n",
        "\n",
        "#         val_reconstructions.append(y[0])\n",
        "\n",
        "#     val_reconstructions = np.array(val_reconstructions)\n",
        "\n",
        "#     # --- Calculate per-feature reconstruction errors on normal data ---\n",
        "#     val_reconstruction_errors = np.abs(val_reconstructions - X_val)\n",
        "\n",
        "#     # --- Define the new threshold using a statistical method ---\n",
        "#     mean_feature_errors = np.mean(val_reconstruction_errors, axis=0)\n",
        "#     std_feature_errors = np.std(val_reconstruction_errors, axis=0)\n",
        "#     new_per_feature_thresholds = mean_feature_errors + std_feature_errors #3 * std_feature_errors\n",
        "\n",
        "#     print(f\"New Per-Feature Thresholds: {new_per_feature_thresholds}\")\n",
        "\n",
        "#     # AE predictions (loop per sample)\n",
        "#     # --- AE Test Predictions ---\n",
        "#     ae_y_pred_probs = []\n",
        "#     for i in range(len(X_test)):\n",
        "#         x = np.expand_dims(X_test[i].astype(np.float32), axis=0)  # Correct: shape [1, num_features]\n",
        "#         if ae_input_details['dtype'] == np.int8:\n",
        "#             xq = np.round(x / in_scale + in_zp).astype(np.int8)\n",
        "#         else:\n",
        "#             xq = x.astype(ae_input_details['dtype'])\n",
        "\n",
        "#         ae_interpreter.set_tensor(ae_input_details['index'], xq)\n",
        "#         ae_interpreter.invoke()\n",
        "#         yq = ae_interpreter.get_tensor(ae_output_details['index'])\n",
        "\n",
        "#         if ae_output_details['dtype'] == np.int8:\n",
        "#             y = (yq.astype(np.float32) - out_zp) * out_scale\n",
        "#         else:\n",
        "#             y = yq.astype(np.float32)\n",
        "\n",
        "#         ae_y_pred_probs.append(y[0])\n",
        "\n",
        "#     ae_y_pred_probs = np.array(ae_y_pred_probs)\n",
        "\n",
        "#     # --- Anomaly Detection ---\n",
        "#     test_reconstruction_errors = np.abs(ae_y_pred_probs - X_test)\n",
        "#     #print(per_feature_thresholds)\n",
        "\n",
        "#     # ae_y_pred = (test_reconstruction_errors > (per_feature_thresholds)).any(axis=1).astype(int)\n",
        "#     ae_y_pred = (test_reconstruction_errors > (new_per_feature_thresholds)).any(axis=1).astype(int)\n",
        "#     malicious_pred_indices = np.flatnonzero(ae_y_pred)\n",
        "\n",
        "#     # End resource tracking\n",
        "#     ae_end_time = time.time()\n",
        "#     ae_mem_after = ae_process.memory_info().rss / (1024*1024)\n",
        "#     ae_cpu_after = psutil.cpu_percent(interval=None)\n",
        "\n",
        "#     # Calculate resource usage\n",
        "#     ae_storage_size_mb = os.path.getsize(ae_file_path) / (1024*1024)\n",
        "#     ae_memory_used_mb = ae_mem_after - ae_mem_before\n",
        "#     ae_cpu_usage_percent = ae_cpu_after - ae_cpu_before\n",
        "#     ae_inference_time_sec = ae_end_time - ae_start_time\n",
        "\n",
        "#     # --- Confusion Matrix ---\n",
        "\n",
        "#     y_test_binary = (y_test != 0).astype(int)# Convert y_test to binary: 0 = Normal, 1 = Malware\n",
        "\n",
        "#     ae_cm = confusion_matrix(y_test_binary, ae_y_pred)\n",
        "#     labels = [\"Normal\", \"Malware\"]\n",
        "\n",
        "#     plt.figure(figsize=(6, 4))\n",
        "#     sns.heatmap(ae_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "#     plt.title(f\"{ae_name} - Confusion Matrix (Anomaly Detection)\")\n",
        "#     plt.xlabel(\"Predicted\")\n",
        "#     plt.ylabel(\"True\")\n",
        "#     plt.show()\n",
        "\n",
        "#     # --- Accuracy & Classification Report ---\n",
        "#     print(\"Accuracy:\", accuracy_score(y_test, ae_y_pred))\n",
        "#     print(\"Classification Report:\")\n",
        "#     print(classification_report(y_test_binary, ae_y_pred, target_names=labels, labels=[0, 1]))\n",
        "\n",
        "\n",
        "#     # --- Malware vs Benign Metrics ---\n",
        "#     # Confusion matrix layout (for binary classification):\n",
        "#     # [[TN, FP],\n",
        "#     #  [FN, TP]]\n",
        "#     tn, fp, fn, tp = ae_cm.ravel()\n",
        "\n",
        "#     total_malware = tp + fn\n",
        "#     total_benign = tn + fp\n",
        "\n",
        "#     malware_identified = tp\n",
        "#     benign_misclassified = fp\n",
        "\n",
        "#     percentage_malware_identified = (malware_identified / total_malware * 100) if total_malware > 0 else 0\n",
        "#     percentage_benign_misclassified = (benign_misclassified / total_benign * 100) if total_benign > 0 else 0\n",
        "\n",
        "#     print(f\"Malware Identified: {malware_identified}/{total_malware} ({percentage_malware_identified:.2f}%)\")\n",
        "#     print(f\"Benign Misclassified: {benign_misclassified}/{total_benign} ({percentage_benign_misclassified:.2f}%)\")\n",
        "#     print(f\"TP: {tp}\\nTN: {tn}\\nFP: {fp}\\nFN: {fn}\")\n",
        "\n",
        "#     # Display Resource Usage\n",
        "#     print(f\"\\n--- Resource Usage for {ae_name} ---\")\n",
        "#     print(f\"Storage size: {ae_storage_size_mb:.2f} MB\")\n",
        "#     print(f\"Memory used during inference: {ae_memory_used_mb:.2f} MB\")\n",
        "#     print(f\"CPU usage change: {ae_cpu_usage_percent:.2f}%\")\n",
        "#     print(f\"Inference time: {ae_inference_time_sec:.4f} sec\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     # --- MLP --- #\n",
        "#     # Loads the TFLite model and gets it ready to make predictions\n",
        "#     mlp_interpreter = tf.lite.Interpreter(model_content=mlp_model_content) # Create interpreter object that will read and run the TFLite model\n",
        "#     mlp_interpreter.allocate_tensors() # Make the interpreter allocate memory\n",
        "#     mlp_input_details = mlp_interpreter.get_input_details()[0] # Get expected shape and data type of the data the model needs to evaluate (built-in method)\n",
        "#     mlp_output_details = mlp_interpreter.get_output_details()[0] #Sshape and data type the model will need to produce the results in\n",
        "\n",
        "#     mlp_in_scale, mlp_in_zp = mlp_input_details['quantization']\n",
        "#     mlp_out_scale, mlp_out_zp = mlp_output_details['quantization']\n",
        "#     print(f\"MLP Input Details: {mlp_input_details}\")\n",
        "#     print(f\"MLP Output Details: {mlp_output_details}\")\n",
        "\n",
        "#     # Start Resource Measurement\n",
        "#     mlp_process = psutil.Process(os.getpid())\n",
        "#     mlp_mem_before = mlp_process.memory_info().rss / (1024 * 1024)  # MB\n",
        "#     mlp_cpu_before = psutil.cpu_percent(interval=None)\n",
        "#     mlp_start_time = time.time()\n",
        "\n",
        "#     # --- MLP Predictions ---\n",
        "#     mlp_y_pred_probs = []\n",
        "#     mlp_X_test = X_test[malicious_pred_indices]\n",
        "#     mlp_y_test = y_test[malicious_pred_indices]\n",
        "\n",
        "#     for i in range(len(mlp_X_test)):\n",
        "#         x = np.expand_dims(mlp_X_test[i].astype(np.float32), axis=0)  # shape [1, num_features]\n",
        "#         if mlp_input_details['dtype'] == np.int8:\n",
        "#             xq = np.round(x / mlp_in_scale + mlp_in_zp).astype(np.int8)\n",
        "#         else:\n",
        "#             xq = x.astype(mlp_input_details['dtype'])\n",
        "\n",
        "#         mlp_interpreter.set_tensor(mlp_input_details['index'], xq)\n",
        "#         mlp_interpreter.invoke()\n",
        "#         yq = mlp_interpreter.get_tensor(mlp_output_details['index'])\n",
        "\n",
        "#         if mlp_output_details['dtype'] == np.int8:\n",
        "#             y = (yq.astype(np.float32) - mlp_out_zp) * mlp_out_scale\n",
        "#         else:\n",
        "#             y = yq.astype(np.float32)\n",
        "\n",
        "#         mlp_y_pred_probs.append(y[0])\n",
        "\n",
        "#     mlp_y_pred_probs = np.array(mlp_y_pred_probs)\n",
        "#     mlp_y_pred = np.argmax(mlp_y_pred_probs, axis=1)\n",
        "\n",
        "#     # End Resource Measurement\n",
        "#     mlp_end_time = time.time()\n",
        "#     mlp_mem_after = mlp_process.memory_info().rss / (1024 * 1024)\n",
        "#     mlp_cpu_after = psutil.cpu_percent(interval=None)\n",
        "\n",
        "#     # Calculate Resource Measurement\n",
        "#     mlp_storage_size_mb = os.path.getsize(mlp_file_path) / (1024 * 1024) # Use the correct file path\n",
        "#     mlp_memory_used_mb = mlp_mem_after - mlp_mem_before\n",
        "#     mlp_cpu_usage_percent = mlp_cpu_after - mlp_cpu_before\n",
        "#     mlp_inference_time_sec = mlp_end_time - mlp_start_time\n",
        "\n",
        "#     # --- Confusion Matrix ---\n",
        "#     cm = confusion_matrix(mlp_y_test, mlp_y_pred)\n",
        "#     reverse_attack_type_map = {v: k for k, v in attack_type_map.items()}\n",
        "#     labels = [reverse_attack_type_map.get(i, f'Unknown {i}') for i in range(cm.shape[0])]\n",
        "\n",
        "#     plt.figure(figsize=(8, 4)) # Create confusion matrix plot\n",
        "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "#     plt.title(f\"{mlp_name} - Confusion Matrix (Classification)\")\n",
        "#     plt.xlabel(\"Predicted\")\n",
        "#     plt.ylabel(\"True\")\n",
        "#     plt.show()\n",
        "\n",
        "#     # Print accuracy & classification report\n",
        "#     print(\"Accuracy:\", accuracy_score(mlp_y_test, mlp_y_pred))\n",
        "#     print(\"Classification Report:\")\n",
        "#     print(classification_report(mlp_y_test, mlp_y_pred, target_names=labels))\n",
        "\n",
        "#     # Calculate malware vs benign sample metrics\n",
        "#     normal_label = attack_type_map.get('Normal Traffic', None)\n",
        "#     if normal_label is not None:\n",
        "\n",
        "#         malware_identified = np.sum(np.diag(cm)) - (cm[normal_label, normal_label] if normal_label in mlp_y_test else 0)\n",
        "#         total_malware = np.sum(cm) - (np.sum(cm[normal_label, :]) if normal_label in mlp_y_test else 0)\n",
        "#         percentage_malware_identified = (malware_identified / total_malware * 100) if total_malware > 0 else 0\n",
        "\n",
        "#         benign_misclassified = (np.sum(cm[normal_label, :]) - cm[normal_label, normal_label]) if normal_label in mlp_y_test else 0\n",
        "#         total_benign = np.sum(cm[normal_label, :]) if normal_label in mlp_y_test else 0\n",
        "#         percentage_benign_misclassified = (benign_misclassified / total_benign * 100) if total_benign > 0 else 0\n",
        "\n",
        "#         print(f\"Malware Identified: {malware_identified}/{total_malware} ({percentage_malware_identified:.2f}%)\")\n",
        "#         print(f\"Benign Misclassified: {benign_misclassified}/{total_benign} ({percentage_benign_misclassified:.2f}%)\")\n",
        "#         print(f\"TP: {malware_identified}, TN: {(cm[normal_label, normal_label] if normal_label in mlp_y_test else 0)}, FP: {benign_misclassified}, FN: {total_malware - malware_identified}\")\n",
        "\n",
        "\n",
        "#     # Display Resource Usage\n",
        "#     print(f\"\\n--- Resource Usage for {mlp_name} ---\")\n",
        "#     print(f\"Storage size: {mlp_storage_size_mb:.2f} MB\")\n",
        "#     print(f\"Memory used during inference: {mlp_memory_used_mb:.2f} MB\")\n",
        "#     print(f\"CPU usage change: {mlp_cpu_usage_percent:.2f}%\")\n",
        "#     print(f\"Inference time: {mlp_inference_time_sec:.4f} sec\")\n",
        "\n",
        "#     # Add a pause to allow CPU to reset\n",
        "#     time.sleep(5) # Pause for 5 seconds"
      ],
      "metadata": {
        "id": "_KsgELCUWNzO"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}